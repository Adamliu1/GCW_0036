{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[COMP0036 Group Assignment: BEAT THE BOOKIE](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Group N - Introduction](#toc0_)\n",
    "\n",
    "We have been assigned to build model(s) that predict the FTR value, which can be Home Win (H), Draw (D) and Away Win (A). The general steps we will be taking to build the model(s) begins with finding a suitable dataset and performing feature engineering on the selected features to be used in the model. This entails creating functions or classes to convert the raw data and transforms it into a format where every match has that historic feature. Then, we perform feature selection to filter out unimportant features, and use the selected features in model(s), and then compare and decide the best performing model. Finally, improve models to get the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [COMP0036 Group Assignment: BEAT THE BOOKIE](#toc1_)    \n",
    "  - [Group N - Introduction](#toc1_1_)    \n",
    "  - [Index](#toc1_2_)    \n",
    "- [Data Import](#toc2_)    \n",
    "  - [Libraries & data source define](#toc2_1_)    \n",
    "    - [Libraries](#toc2_1_1_)    \n",
    "    - [RUNNING Config (GPU/CPU)](#toc2_1_2_)    \n",
    "    - [FLAGS TO DISABLE false positive warnings](#toc2_1_3_)    \n",
    "    - [Data Source path](#toc2_1_4_)    \n",
    "  - [Raw data loading and inspection](#toc2_2_)    \n",
    "- [Data Transformation & Exploration](#toc3_)    \n",
    "  - [Initial transformations](#toc3_1_)    \n",
    "    - [Replacing 'Date',strings with DateTime objects](#toc3_1_1_)    \n",
    "    - [Adding standings and rankings data to the Dataframe](#toc3_1_2_)    \n",
    "    - [Adding manager data to the Dataframe](#toc3_1_3_)    \n",
    "    - [Encoding Categorical Data](#toc3_1_4_)    \n",
    "  - [Data Exploration](#toc3_2_)    \n",
    "    - [Analyse Match Data](#toc3_2_1_)    \n",
    "    - [Analyse Team Rankings across the years](#toc3_2_2_)    \n",
    "    - [Analysing a Team Manager Performance Across The 5 Most Recent Seasons](#toc3_2_3_)    \n",
    "- [Feature Engineering](#toc4_)    \n",
    "    - [Adding Average Past Match Statistics & Past Season % Number Of Wins](#toc4_1_1_)    \n",
    "    - [Adding Expected Goals](#toc4_1_2_)    \n",
    "    - [Removing Pre-encoded Data](#toc4_1_3_)    \n",
    "  - [Breakdown of Features In The Dataframe/Dataset](#toc4_2_)    \n",
    "  - [Final Dataframe containing all features](#toc4_3_)    \n",
    "- [Feature Selection](#toc5_)    \n",
    "  - [Manual Feature Selection & Motivations](#toc5_1_)    \n",
    "      - [Preparation](#toc5_1_1_1_)    \n",
    "    - [Introduction](#toc5_1_2_)    \n",
    "    - [FEATURE SET 1 - Baseline Classifier: Random Guesses](#toc5_1_3_)    \n",
    "      - [Motivation](#toc5_1_3_1_)    \n",
    "      - [Create Design Matrix](#toc5_1_3_2_)    \n",
    "      - [Evaluate without cross-validation](#toc5_1_3_3_)    \n",
    "    - [FEATURE SET 2 - Simple: (HomeTeam, AwayTeam and Month)](#toc5_1_4_)    \n",
    "      - [Motivation](#toc5_1_4_1_)    \n",
    "      - [Create Design Matrix](#toc5_1_4_2_)    \n",
    "    - [FEATURE SET 3 - Included Averages (All Seasons + Recent Season)](#toc5_1_5_)    \n",
    "      - [Motivation](#toc5_1_5_1_)    \n",
    "      - [Create Design Matrix](#toc5_1_5_2_)    \n",
    "    - [FEATURE SET 4 – Complex: Included Expected Goals, Number of Past Wins (Home & Away)](#toc5_1_6_)    \n",
    "      - [Motivation](#toc5_1_6_1_)    \n",
    "      - [Create Design Matrix](#toc5_1_6_2_)    \n",
    "    - [FEATURE SET 5 – Complex: Included Team Managers and Previous Season Standings](#toc5_1_7_)    \n",
    "      - [Motivation](#toc5_1_7_1_)    \n",
    "      - [Create Design Matrix](#toc5_1_7_2_)    \n",
    "  - [Automatic Feature Selection](#toc5_2_)    \n",
    "    - [Choose K Best Features](#toc5_2_1_)    \n",
    "      - [Auto Select Features](#toc5_2_1_1_)    \n",
    "      - [Create Design Matrix](#toc5_2_1_2_)    \n",
    "    - [Recursive Feature Elimination](#toc5_2_2_)    \n",
    "      - [Auto Select Features - NOTE: EXTREMELY SLOW - Careful when running this cell](#toc5_2_2_1_)    \n",
    "      - [Evaluation on CV results of RFE method](#toc5_2_2_2_)    \n",
    "      - [Create Design Matrix](#toc5_2_2_3_)    \n",
    "    - [Using Select From Model (L1-based feature selection)](#toc5_2_3_)    \n",
    "      - [Auto Select Features](#toc5_2_3_1_)    \n",
    "      - [Create Design Matrix](#toc5_2_3_2_)    \n",
    "    - [Tree Based Model (select from model)](#toc5_2_4_)    \n",
    "      - [Auto Select Features](#toc5_2_4_1_)    \n",
    "      - [Create Design Matrix](#toc5_2_4_2_)    \n",
    "    - [Sequential Feature Selection](#toc5_2_5_)    \n",
    "      - [Auto Select Features](#toc5_2_5_1_)    \n",
    "      - [Create Design Matrix](#toc5_2_5_2_)    \n",
    "- [Feature Extraction](#toc6_)    \n",
    "- [Feature extraction using dimensionality reduction](#toc7_)    \n",
    "  - [PCA](#toc7_1_)    \n",
    "  - [Autoencoder](#toc7_2_)    \n",
    "- [Methodology Overview](#toc8_)    \n",
    "  - [Auxiliary Functions + Classifier interfaces](#toc8_1_)    \n",
    "    - [Evaluation helpers](#toc8_1_1_)    \n",
    "    - [Plotting helpers](#toc8_1_2_)    \n",
    "    - [Metrics and classifiers](#toc8_1_3_)    \n",
    "      - [scoring metrics and define cross validation data split](#toc8_1_3_1_)    \n",
    "        - [helper function for producing report](#toc8_1_3_1_1_)    \n",
    "      - [Classifiers](#toc8_1_3_2_)    \n",
    "        - [Random Guesses](#toc8_1_3_2_1_)    \n",
    "        - [Decision Tree Classifier](#toc8_1_3_2_2_)    \n",
    "        - [Random Forest Classifier](#toc8_1_3_2_3_)    \n",
    "        - [K-Nearest Neighbours (KNN) Classifier](#toc8_1_3_2_4_)    \n",
    "        - [Support Vector Machine (SVM) Classifier](#toc8_1_3_2_5_)    \n",
    "        - [XGB](#toc8_1_3_2_6_)    \n",
    "        - [Neural Network](#toc8_1_3_2_7_)    \n",
    "        - [Feature set interfaces](#toc8_1_3_2_8_)    \n",
    "- [Model Training & Validation](#toc9_)    \n",
    "  - [Manual Feature Sets](#toc9_1_)    \n",
    "    - [Feature Set 1](#toc9_1_1_)    \n",
    "      - [Create design matrix](#toc9_1_1_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_1_1_2_)    \n",
    "      - [Summary](#toc9_1_1_3_)    \n",
    "    - [Feature Set 2](#toc9_1_2_)    \n",
    "      - [Create design matrix](#toc9_1_2_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_1_2_2_)    \n",
    "      - [Summary](#toc9_1_2_3_)    \n",
    "    - [Feature Set 3](#toc9_1_3_)    \n",
    "      - [Create design matrix](#toc9_1_3_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_1_3_2_)    \n",
    "      - [Summary](#toc9_1_3_3_)    \n",
    "    - [Feature Set 4](#toc9_1_4_)    \n",
    "      - [Create design matrix](#toc9_1_4_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_1_4_2_)    \n",
    "      - [Summary](#toc9_1_4_3_)    \n",
    "    - [Feature Set 5](#toc9_1_5_)    \n",
    "      - [Create design matrix](#toc9_1_5_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_1_5_2_)    \n",
    "      - [Summary](#toc9_1_5_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_1_5_4_)    \n",
    "      - [Summary (CV)](#toc9_1_5_5_)    \n",
    "  - [Automatically Selected Feature Sets](#toc9_2_)    \n",
    "    - [Choose K Best Features](#toc9_2_1_)    \n",
    "      - [Create design matrix](#toc9_2_1_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_2_1_2_)    \n",
    "      - [Summary](#toc9_2_1_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_2_1_4_)    \n",
    "      - [Summary (CV)](#toc9_2_1_5_)    \n",
    "    - [Recursive Feature Elimination](#toc9_2_2_)    \n",
    "      - [Create design matrix](#toc9_2_2_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_2_2_2_)    \n",
    "      - [Summary](#toc9_2_2_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_2_2_4_)    \n",
    "      - [Summary (CV)](#toc9_2_2_5_)    \n",
    "    - [Using Select From Model (L1-based feature selection)](#toc9_2_3_)    \n",
    "      - [Create design matrix](#toc9_2_3_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_2_3_2_)    \n",
    "      - [Summary](#toc9_2_3_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_2_3_4_)    \n",
    "      - [Summary (CV)](#toc9_2_3_5_)    \n",
    "    - [Tree Based Model (select from model)](#toc9_2_4_)    \n",
    "      - [Create design matrix](#toc9_2_4_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_2_4_2_)    \n",
    "      - [Summary](#toc9_2_4_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_2_4_4_)    \n",
    "      - [Summary (CV)](#toc9_2_4_5_)    \n",
    "    - [Sequential Feature Selection](#toc9_2_5_)    \n",
    "      - [Create design matrix](#toc9_2_5_1_)    \n",
    "      - [Evaluate without cross validation](#toc9_2_5_2_)    \n",
    "      - [Summary](#toc9_2_5_3_)    \n",
    "      - [Evaluate with cross validation](#toc9_2_5_4_)    \n",
    "      - [Summary (CV)](#toc9_2_5_5_)    \n",
    "  - [Feature Extracted Feature Sets](#toc9_3_)    \n",
    "- [Results](#toc10_)    \n",
    "- [Final Predictions on Test Set](#toc11_)    \n",
    "- [Conclusions](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Index](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Data Import](#toc0_)\n",
    "Here we import the libraries and define the data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Libraries & data source define](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import operator\n",
    "import random\n",
    "from calendar import month_name\n",
    "from pandas.core.common import random_state\n",
    "\n",
    "from typing import Any, Callable\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[RUNNING Config (GPU/CPU)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out this cell if not using GPU acceleration\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpu_devices)\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# This statement is used to log if tf is using GPU\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[FLAGS TO DISABLE false positive warnings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Data Source path](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName_matchData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/epl-full-training.csv'\n",
    "dirName_rankingData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/EPL%20Standings%202000-2022.csv'\n",
    "dirName_managerData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/epl-manager-data-V3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Raw data loading and inspection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the main csv file\n",
    "df_epl = pd.read_csv(dirName_matchData)\n",
    "# Check the raw data\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the standings csv file\n",
    "df_ranking = pd.read_csv(dirName_rankingData)\n",
    "df_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the manager csv file\n",
    "df_manager = pd.read_csv(dirName_managerData, encoding=\"ISO-8859-1\")\n",
    "df_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Transformation & Exploration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Initial transformations](#toc0_)\n",
    "\n",
    "### <a id='toc3_1_1_'></a>[Replacing 'Date',strings with DateTime objects](#toc0_)\n",
    "\n",
    "In the raw dataset, the `'Date'` column has a `string` type. Converting it to DateTime object will allow easier usage. It also allows for extracting our first features - day, month and year of the game. We will analyse their importance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"], dayfirst=True)\n",
    "\n",
    "df_epl[\"Day\"] = df_epl[\"Date\"].dt.day\n",
    "df_epl[\"Month\"] = df_epl[\"Date\"].dt.month \n",
    "df_epl[\"Year\"] = df_epl[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Adding standings and rankings data to the Dataframe](#toc0_)\n",
    "\n",
    "The following snippet is used to combine match detailed dataframe - `df_epl`, with team seasonal standings and rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - HELPER FUNCTIONS:\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if int(date.month) <= 7:\n",
    "      return datetime(int(date.year)-1, 8, 1)\n",
    "    return datetime(int(date.year), 8, 1)\n",
    "\n",
    "\n",
    "# PART 2 - ADDING STANDINGS DATA TO df_epl:\n",
    "\n",
    "homeTeamRankings = []\n",
    "awayTeamRankings = []\n",
    "diffRankings = []\n",
    "\n",
    "for index, row in df_epl.iterrows():\n",
    "    season = get_season_start_date(row[\"Date\"])\n",
    "    prev_season = str(season.year-1) + \"-\" + str(season.year)[-2:] # from df_ranking\n",
    "    homeTeam = row[\"HomeTeam\"]    \n",
    "    awayTeam = row[\"AwayTeam\"]\n",
    "\n",
    "    #filter dataframe ranking for year=season and hometeam=homeTeam\n",
    "    df_epl_train_filtered_H = df_ranking.copy()\n",
    "    df_epl_train_filtered_H = df_epl_train_filtered_H[(df_ranking.Season==prev_season) & (df_ranking.Team==homeTeam)]\n",
    "\n",
    "    #filter dataframe ranking for year=season and awayTeam=awayTeam\n",
    "    df_epl_train_filtered_A = df_ranking.copy()\n",
    "    df_epl_train_filtered_A = df_epl_train_filtered_A[(df_ranking.Season==prev_season) & (df_ranking.Team==awayTeam)]\n",
    "    \n",
    "    # To keep df consistent, we add Nan to row, so we could remove it later by using df.dropna()\n",
    "    # TODO: refactory\n",
    "    # 18 represents team that been promoted from previous league this year. Hence, they don't have ranking from previous league\n",
    "    # Pos is embbed inside pd.Series object, use values to access values of series.\n",
    "    # Because Pos should be a int, hence, values always has length 1 or 0 (no data)\n",
    "    if df_epl_train_filtered_H.empty and df_epl_train_filtered_A.empty:\n",
    "        homeRanking = 18\n",
    "        awayRanking = 18\n",
    "    elif df_epl_train_filtered_H.empty and (not df_epl_train_filtered_A.empty):\n",
    "        homeRanking = 18\n",
    "        awayRanking = df_epl_train_filtered_A['Pos'].values[0]\n",
    "    elif (not df_epl_train_filtered_H.empty) and df_epl_train_filtered_A.empty:\n",
    "        homeRanking = df_epl_train_filtered_H['Pos'].values[0]\n",
    "        awayRanking = 18\n",
    "    else:\n",
    "        homeRanking = df_epl_train_filtered_H['Pos'].values[0]\n",
    "        awayRanking = df_epl_train_filtered_A['Pos'].values[0]\n",
    "\n",
    "    homeTeamRankings.append(homeRanking)\n",
    "    awayTeamRankings.append(awayRanking)\n",
    "    diffRankings.append(homeRanking-awayRanking)\n",
    "\n",
    "df_epl['HomeStanding_PrevSeason'] = homeTeamRankings\n",
    "df_epl['AwayStanding_PrevSeason'] = awayTeamRankings\n",
    "df_epl['DiffStanding_PrevSeason'] = diffRankings\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Adding manager data to the Dataframe](#toc0_)\n",
    "\n",
    "The following snippet is used to combine the existing dataframe - `df_epl`, with the third dataset consisting team managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - CLEAN MANAGER DATA:\n",
    "\n",
    "# Check the original manager data - note nan's in 'End Date',indicate that manager is still at club\n",
    "# Here we update the nan values in the dataframe to the current date today\n",
    "updatedEndDates = []\n",
    "now = datetime.now() # Get current time\n",
    "today = now.strftime(\"%d/%m/%Y\")\n",
    "for index, row in df_manager.iterrows():\n",
    "    date =  row['End Date']\n",
    "    if pd.isnull(date):\n",
    "        updatedEndDates.append(today)\n",
    "    else:\n",
    "        updatedEndDates.append(date)\n",
    "        \n",
    "# Add these updated End_Dates to the dataframe\n",
    "df_manager['End_Date'] = updatedEndDates\n",
    "\n",
    "# Change the names of column index to include '_',instead of space and convert string dates into datetime objects\n",
    "df_manager['Start_Date'] = pd.to_datetime(df_manager[\"Start Date\"], dayfirst=True)\n",
    "df_manager['End_Date'] = pd.to_datetime(df_manager[\"End_Date\"], dayfirst=True)\n",
    "\n",
    "\n",
    "\n",
    "# PART 2 - ADDING MANAGER DATA TO df_epl:\n",
    "\n",
    "homeTeamManagers = []\n",
    "awayTeamManagers = []\n",
    "\n",
    "# Here for each row in the df_epl we will add the manager for home and away team\n",
    "for index, row in df_epl.iterrows():\n",
    "    date =  row['Date']\n",
    "    homeTeam = row[\"HomeTeam\"]\n",
    "    awayTeam = row[\"AwayTeam\"]\n",
    "\n",
    "    #filter dataframe ranking for date between start and end of managers and hometeam=homeTeam\n",
    "    df_epl_train_filtered_H = df_manager.copy()\n",
    "    df_epl_train_filtered_H = df_epl_train_filtered_H[(df_manager.Start_Date<=date) & (df_manager.End_Date>=date) & (df_manager.Club==homeTeam)]\n",
    "\n",
    "    #filter dataframe ranking for date between start and end of managers and awayTeam=awayTeam\n",
    "    df_epl_train_filtered_A = df_manager.copy()\n",
    "    df_epl_train_filtered_A = df_epl_train_filtered_A[(df_manager.Start_Date<=date) & (df_manager.End_Date>=date) & (df_manager.Club==awayTeam)]\n",
    "    \n",
    "    # NOTE - we get some managers as anonymous - this is happening when there is a change of manager (old manager has left, new manager has not started yet) and a match takes place\n",
    "    if df_epl_train_filtered_H.empty:\n",
    "        homeManager = \"Anonymous\"\n",
    "    else:\n",
    "        homeManager = df_epl_train_filtered_H['Name'].values[0]\n",
    "        \n",
    "    if df_epl_train_filtered_A.empty:\n",
    "        awayManager = \"Anonymous\"\n",
    "    else:\n",
    "        awayManager = df_epl_train_filtered_A['Name'].values[0]\n",
    "    \n",
    "    homeTeamManagers.append(homeManager)\n",
    "    awayTeamManagers.append(awayManager)\n",
    "\n",
    "df_epl['Home_Manager'] = homeTeamManagers\n",
    "df_epl['Away_Manager'] = awayTeamManagers\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_4_'></a>[Encoding Categorical Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models accept only numbers as their input, so we need to encode all categorical data like team names, referees and managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if home teams are the same as away teams, same for managers\n",
    "print(set(df_epl['AwayTeam'].unique()) == set(df_epl['HomeTeam'].unique()))\n",
    "print(set(df_epl['Home_Manager'].unique()) == set(df_epl['Away_Manager'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categorical string data to integer values\n",
    "team_encoder = LabelEncoder()\n",
    "df_epl[\"HomeTeam_Enc\"] = team_encoder.fit_transform(df_epl['HomeTeam'])\n",
    "df_epl[\"AwayTeam_Enc\"] = team_encoder.transform(df_epl['AwayTeam'])\n",
    "\n",
    "referee_encoder = LabelEncoder()\n",
    "df_epl[\"Referee_Enc\"] = referee_encoder.fit_transform(df_epl['Referee'])\n",
    "\n",
    "FTR_encoder = LabelEncoder()\n",
    "df_epl[\"FTR_Enc\"] = FTR_encoder.fit_transform(df_epl[\"FTR\"])\n",
    "df_epl[\"HTR_Enc\"] = FTR_encoder.transform(df_epl[\"HTR\"])\n",
    "\n",
    "manager_encoder = LabelEncoder()\n",
    "manager_encoder.fit(list(set(df_epl['Home_Manager'].unique()).union(set(df_epl['Away_Manager'].unique()))))\n",
    "df_epl[\"Home_Manager_Enc\"] = manager_encoder.transform(df_epl['Home_Manager'])\n",
    "df_epl[\"Away_Manager_Enc\"] = manager_encoder.transform(df_epl['Away_Manager'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Data Exploration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Analyse Match Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting and engineering features, we explore the dataset to learn about its specifics. Firstly, we check for visible outliers, existing NaN values and have a look at the output of the `.describe()` summary for all numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are no NaN values in the dataset. Quick glance at other statistcs gives hope that the data is not corrupted. Let's check if this classification problem can be considered balanced by plotting a histogram of classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_epl, x='FTR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df = df_epl.describe()\n",
    "# Graph inspired by https://armantee.github.io/predicting/\n",
    "\n",
    "home_features = ['FTHG','HTHG','HS','HST','HC','HF','HY','HR']\n",
    "away_features = ['FTAG','HTAG','AS','AST','AC','AF','AY','AR']\n",
    "fig=plt.figure(figsize=(18, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# width of the bars\n",
    "barWidth = 0.2\n",
    "bars1 = np.array(describe_df.iloc[1, :][home_features]).flatten()\n",
    "bars2 = np.array(describe_df.iloc[1, :][away_features]).flatten()\n",
    "yer1 = np.array(describe_df.iloc[2, :][home_features]).flatten()\n",
    "yer2 = np.array(describe_df.iloc[2, :][away_features]).flatten()\n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1.flatten()))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "# Create blue bars\n",
    "plt.bar(r1, bars1, width = barWidth, color = 'orange', edgecolor = 'black', yerr=yer1, capsize=7, label='Home team')\n",
    "# Create cyan bars\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'green', edgecolor = 'black', yerr=yer2, capsize=7, label='Away team')\n",
    "# general layout\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['Goals Scored', 'Half-time Goals Scored', 'Shots','Shots on target', 'Corners','Fouls','Yellow Cards','Red Cards'])\n",
    "plt.ylabel('Average Value')\n",
    "plt.ylim(0)\n",
    "plt.title(\"Average values for features split between home and away team.\")\n",
    "plt.legend()\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most of standard deviation errors overlap greatly, the home advantage is clearly visible. This needs to be taken into account while working on classifiers. Shots and Shots on target may be especially great indicators for the result. Let's verify this and explore the correlation matrix for the existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(df_epl.corr(numeric_only=\"False\"), cmap=\"PiYG\", annot= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_dict = []\n",
    "for i in range(2002, 2023):\n",
    "    grouped_wins = df_epl[(df_epl['FTR'] == 'H') & (df_epl['Year'] == i)].groupby('HomeTeam')\n",
    "    grouped_all = df_epl[(df_epl['Year'] == i)].groupby('HomeTeam')\n",
    "    \n",
    "    year_dict.append(dict(grouped_wins['FTR'].count()/grouped_all['FTR'].count()))\n",
    "\n",
    "def get_team_stats(team_name):\n",
    "    team = []\n",
    "    for year in range(len(year_dict)):\n",
    "        try:\n",
    "            value = year_dict[year][team_name]\n",
    "            if np.isnan(value):\n",
    "                value = 0\n",
    "        except KeyError:\n",
    "            value = 0\n",
    "        team.append(value)\n",
    "    return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(14, 3, figsize=(12, 40), constrained_layout=True)\n",
    "\n",
    "for i, team_name in enumerate(df_epl['HomeTeam'].unique()):\n",
    "    ax[i//3][i%3].set_title(team_name)\n",
    "    ax[i//3][i%3].grid()\n",
    "    # ax[i//3][i%3].set_xticks(np.arange(2002, 2023, step=1))\n",
    "    ax[i//3][i%3].set_ylim(ymin=0)\n",
    "    y = get_team_stats(team_name)\n",
    "    x = np.array((range(2002, 2023)))\n",
    "    ax[i//3][i%3].plot(x, y, '-o')\n",
    "    if 0 not in set(y):\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        ax[i//3][i%3].plot(x, a*x+b)\n",
    "fig.suptitle(\"Team performance over years - percentage of won matches\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that most of the teams did not play for the full 20 past years. This is likely caused by league promotions and relegations. \n",
    "\n",
    "Let's see if a time of a year has an influence on a match outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "month_names = {}\n",
    "for month in range(1, 13):\n",
    "    h = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'H')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    d = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'D')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    a = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'A')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    counts[month_name[month]] = [h, a, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html#sphx-glr-gallery-lines-bars-and-markers-horizontal-barchart-distribution-py\n",
    "\n",
    "category_names = ['H', 'A', 'D']\n",
    "results = counts\n",
    "\n",
    "labels = list(results.keys())\n",
    "data = np.array(list(results.values()))\n",
    "data_cum = data.cumsum(axis=1)\n",
    "category_colors = plt.colormaps['twilight'](\n",
    "    np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "    widths = data[:, i]\n",
    "    starts = data_cum[:, i] - widths\n",
    "    rects = ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                    label=colname, color=color)\n",
    "\n",
    "    r, g, b, _ = color\n",
    "    text_color = 'white',if r * g * b < 0.5 else 'darkgrey'\n",
    "    ax.bar_label(rects, label_type='center', fmt='%.1f', color=text_color)\n",
    "    \n",
    "ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 0),\n",
    "            loc='upper left', fontsize='small')\n",
    "plt.title(\"Average match results (%) w.r.t. months across years 2000-2022\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_epl, x='Month', binwidth=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it looks like results across months are the similar, apart from June/July and March. This may be the outcome of much less matches being played during those months. Let's investigate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the number of matches won by a team changes across years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Analyse Team Rankings across the years](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_standings = []\n",
    "\n",
    "def get_unique_entries(df_ranking, col):\n",
    "    temp = list(df_ranking[col])\n",
    "    result = list(set(temp))\n",
    "    return result\n",
    "\n",
    "seasons = sorted(get_unique_entries(df_ranking, 'Season'))\n",
    "teams = sorted(get_unique_entries(df_ranking, 'Team'))\n",
    "\n",
    "def get_team_standings(team, df_ranking):\n",
    "    team_standing = {}\n",
    "    rows = df_ranking.loc[df_ranking['Team'] == team]\n",
    "    for index, row in rows.iterrows():\n",
    "        team_standing[row['Season']] = int(row['Pos'])\n",
    "    for season in seasons:\n",
    "        if season not in team_standing.keys():\n",
    "            team_standing[season] = 0\n",
    "    return team_standing\n",
    "\n",
    "for team in teams:\n",
    "    team_standings.append(get_team_standings(team, df_ranking))\n",
    "\n",
    "fig, ax = plt.subplots(len(teams)//3+1, 3, figsize=(12, 40), constrained_layout=True)\n",
    "\n",
    "for i in range(len(teams)):\n",
    "    ax[i//3][i%3].set_title(teams[i])\n",
    "    ax[i//3][i%3].grid()\n",
    "#     ax[i//3][i%3].set_ylim(ymin=0)\n",
    "    y = np.array(list(team_standings[i].values()))\n",
    "    x = np.array(list(range(0, len(list(team_standings[i].values())))))\n",
    "    ax[i//3][i%3].plot(x, y, '-o')\n",
    "\n",
    "fig.suptitle(\"Team performance over years - rankings at the end of each season\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[Analysing a Team Manager Performance Across The 5 Most Recent Seasons](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_manager_points(teams, df_season):\n",
    "    for index, row in df_season.iterrows():\n",
    "        Date, HomeTeam, AwayTeam, FTR, Home_Manager, Away_Manager = row\n",
    "        if HomeTeam not in teams.keys():\n",
    "            teams[HomeTeam] = {}\n",
    "        if AwayTeam not in teams.keys():  \n",
    "            teams[AwayTeam] = {}\n",
    "\n",
    "    for index, row in df_season.iterrows():\n",
    "        Date, HomeTeam, AwayTeam, FTR, Home_Manager, Away_Manager = row\n",
    "        if Home_Manager not in teams[HomeTeam].keys():\n",
    "            teams[HomeTeam][Home_Manager] = result_points(FTR, 'Home')\n",
    "        else:\n",
    "            teams[HomeTeam][Home_Manager] = teams[HomeTeam][Home_Manager] + result_points(FTR, 'Home')\n",
    "\n",
    "        if Away_Manager not in teams[AwayTeam].keys():  \n",
    "            teams[AwayTeam][Away_Manager] = result_points(FTR, 'Away')\n",
    "        else:\n",
    "            teams[AwayTeam][Away_Manager] = teams[AwayTeam][Away_Manager] + result_points(FTR, 'Away')\n",
    "    return teams\n",
    "\n",
    "# Filter the dataframe to include only rows where Date , eg 2017-2018\n",
    "start_years = ['2017', '2018', '2019',, '2020', '2021']\n",
    "end_years = ['2018', '2019',, '2020', '2021', '2022']\n",
    "\n",
    "season1718 = {}\n",
    "season1819 = {}\n",
    "season1920 = {}\n",
    "season2021 = {}\n",
    "season2122 = {}\n",
    "\n",
    "seasons = [season1718, season1819,season1920,season2021,season2122]\n",
    "\n",
    "home_point_system = {\n",
    "    'H':3,\n",
    "    'A':0,\n",
    "    'D':1\n",
    "}\n",
    "\n",
    "away_point_system = {\n",
    "    'H':0,\n",
    "    'A':3,\n",
    "    'D':1\n",
    "}\n",
    "\n",
    "def result_points(FTR, Team):\n",
    "    if Team == 'Home':\n",
    "        return home_point_system[FTR]\n",
    "    else:\n",
    "        return away_point_system[FTR]\n",
    "\n",
    "def get_data_by_season(start_date, end_date, df_main):\n",
    "    df_season = df_main.copy()\n",
    "    df_season = df_season[(df_season.Date>=start_date) & (df_season.Date<=end_date)]\n",
    "    df_season = df_season.drop(columns=['Div', 'FTHG', 'FTAG', 'HTHG',\n",
    "       'HTAG', 'HTR', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC',\n",
    "       'AC', 'HY', 'AY', 'HR', 'AR', 'Day', 'Month', 'Year',\n",
    "       'HomeStanding_PrevSeason', 'AwayStanding_PrevSeason',\n",
    "       'DiffStanding_PrevSeason',\n",
    "       'HomeTeam_Enc', 'AwayTeam_Enc', 'Referee_Enc', 'FTR_Enc', 'HTR_Enc',\n",
    "       'Home_Manager_Enc', 'Away_Manager_Enc'])\n",
    "    return df_season\n",
    "\n",
    "for i in range(len(start_years)):\n",
    "    start_year = start_years[i]\n",
    "    end_year = end_years[i]\n",
    "    start_date = pd.to_datetime('01/08/',+ start_year, dayfirst=True)  \n",
    "    end_date = pd.to_datetime('01/08/'+ end_year, dayfirst=True) \n",
    "    df_season = get_data_by_season(start_date, end_date, df_epl)\n",
    "    seasons[i] = calculate_manager_points(seasons[i], df_season)\n",
    "\n",
    "df_teams_2017_2022 = get_data_by_season('01/08/2017', '01/08/2022', df_epl)\n",
    "\n",
    "set_Home = set(df_teams_2017_2022['HomeTeam'])\n",
    "set_Away = set(df_teams_2017_2022['AwayTeam'])\n",
    "resultList= list(set_Home | set_Away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData = {}\n",
    "\n",
    "for team in resultList:\n",
    "    scores = {}\n",
    "    for i in range(len(seasons)):\n",
    "        currentSeason = seasons[i]\n",
    "        if team in currentSeason.keys(): \n",
    "            currentTeam = currentSeason[team]\n",
    "            seasonKey = start_years[i]\n",
    "            if seasonKey not in scores.keys():\n",
    "                scores[seasonKey] = {}\n",
    "                scores[seasonKey] = currentTeam\n",
    "    plotData[team] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "df_names = []\n",
    "\n",
    "for key, value in plotData.items():\n",
    "    if bool(value):\n",
    "        df_pre = pd.DataFrame(value)\n",
    "        df = df_pre.fillna(0)\n",
    "        df_list.append(df.to_dict())\n",
    "        df_names.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(df_names)//3 + 1, 3, figsize=(35, 40), constrained_layout=True)\n",
    "for i in range(len(df_names)):\n",
    "    plotData = pd.DataFrame(df_list[i])\n",
    "    plotData.transpose().plot.bar(ax=axes[i//3,i%3], title=df_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Adding Average Past Match Statistics & Past Season % Number Of Wins](#toc0_)\n",
    "\n",
    "_HISTORY -> We add averages of past stats between the specific two teams in question. We obtain these stats (for each row of df_epl) by filtering the df_epl dataframe for matches ONLY between HomeTeam and AwayTeam that took place before the match date. Then take an average of the columns (with stats) like HR, AR, etc (of filtered dataframe). This happens for each row. This will provide us with the average past stats for games played in the past between the specific two teams.\n",
    "\n",
    "_AVG -> We add averages of past stats between the for each of two teams in question. We obtain these stats (for each row of df_epl) by filtering the df_epl dataframe for matches between HomeTeam against ALL other teams that took place before the match date in that current season. Similar is done for the AwayTeam. Then take an average of the columns (with stats) like HR, AR, etc (of the filtered dataframe). This process happens for each row. This will provide us with the average past stats for the all HomeTeam games in that season and all AwayTeam games in that season.\n",
    "\n",
    "HW_AVG & AW_AVG -> The number of past wins are calculated by summing the number of wins by the team in the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - THESE ARE HELPER FUNCTIONS WE NEED:\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if date.month <= 7:\n",
    "        return datetime(date.year-1, 8, 1)\n",
    "    return datetime(date.year-0, 8, 1)\n",
    "\n",
    "def filter_dataframe_by_hometeam_history(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_history(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_hometeam_recent_season(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_recent_season(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "# This function takes as input the filtered dataframe from previous cell, features to average and a dictionary,it then appends an average of each feature to the dictionary\n",
    "def average_columns(avg_features, filtered_df):\n",
    "    for feature in avg_features.keys():\n",
    "        df_col_means = filtered_df[feature].mean()\n",
    "        avg_features[feature].append(df_col_means)\n",
    "        \n",
    "# This function takes as input a filtered dataframe from previous cell, and a list, it then appends the % number of home/away wins in past\n",
    "def find_number_of_wins(number_of_wins_list, filtered_df, team):\n",
    "    df_filtered_ftr = filtered_df.copy()\n",
    "    total_games = df_filtered_ftr.shape[0]\n",
    "    if total_games == 0:\n",
    "        number_of_wins_list.append(np.nan)\n",
    "        return\n",
    "    number_of_wins = df_filtered_ftr[(df_filtered_ftr.FTR==team)].shape[0]\n",
    "    number_of_wins_list.append(number_of_wins/total_games)\n",
    "\n",
    "        \n",
    "# PART 2 - CREATE FEATURES & ADDING THEM TO DATAFRAME:\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_hist = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_hist = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_recent = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_recent = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "number_of_wins_HOME = []\n",
    "number_of_wins_AWAY = []\n",
    "\n",
    "\n",
    "# Run the two functions on each row of the df_epl and fill the dictionary\n",
    "for index, row in df_epl.iterrows():\n",
    "    # Filter the dataframe to only show matches played between those teams and before the certain date\n",
    "    df_epl_train_average_hometeam_history = filter_dataframe_by_hometeam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_history = filter_dataframe_by_awayteam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_hometeam_recent_season = filter_dataframe_by_hometeam_recent_season(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_recent_season = filter_dataframe_by_awayteam_recent_season(df_epl, row[\"Date\"],row[\"AwayTeam_Enc\"])\n",
    "    # Get averages from the filtered dataframe and add to the dictionary\n",
    "    average_columns(avg_features_home_hist, df_epl_train_average_hometeam_history)\n",
    "    average_columns(avg_features_away_hist, df_epl_train_average_awayteam_history)\n",
    "    average_columns(avg_features_home_recent, df_epl_train_average_hometeam_recent_season)\n",
    "    average_columns(avg_features_away_recent, df_epl_train_average_awayteam_recent_season)\n",
    "    # Get number_of_wins from the filtered dataframe and add to list\n",
    "    find_number_of_wins(number_of_wins_HOME, df_epl_train_average_hometeam_recent_season, \"H\")\n",
    "    find_number_of_wins(number_of_wins_AWAY, df_epl_train_average_awayteam_recent_season, \"A\")\n",
    "\n",
    "    \n",
    "# Add features to dataframe\n",
    "for feature in avg_features_home_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "\n",
    "# Add features to dataframe\n",
    "for feature in avg_features_away_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_home_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_away_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "    \n",
    "# Add the past % number of wins\n",
    "df_epl[\"HW_AVG\"] = number_of_wins_HOME\n",
    "df_epl[\"AW_AVG\"] = number_of_wins_AWAY\n",
    "\n",
    "\n",
    "# Drop any rows with nan\n",
    "df_epl = df_epl.dropna()\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Adding Expected Goals](#toc0_)\n",
    "\n",
    "The expected goals for each team are calculated using a polynomial regression classifier that is trained using  some of the average past statistics. Then for each row of df_epl, we predict the expected goals for HomeTeam and AwayTeam using the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - CREATE EXPECTED GOALS REGRESSION MODEL:\n",
    "\n",
    "# Here we aim to create a 'expected or predicted goals for a HomeTeam',feature based upon past average stats\n",
    "min_mse_home = float('inf')\n",
    "min_mse_away = float('inf')\n",
    "\n",
    "# Create the design matrix\n",
    "X_H = df_epl.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'FTHG_AVG', 'HTHG_AVG', 'HS_AVG']].values\n",
    "y_H = df_epl.loc[:,'FTHG'].values\n",
    "X_H_train, X_H_test, y_H_train, y_H_test = model_selection.train_test_split(X_H, y_H, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Similar idea for AwayTeam\n",
    "X_A = df_epl.loc[:,['Day', 'Month', 'AwayTeam_Enc', 'FTAG_AVG', 'HTAG_AVG', 'AS_AVG']].values\n",
    "y_A = df_epl.loc[:,'FTAG'].values\n",
    "X_A_train, X_A_test, y_A_train, y_A_test = model_selection.train_test_split(X_A, y_A, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Here we use a polynomial regression classifier - and select best order:\n",
    "for i in range(1,5):\n",
    "    # Select order\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "\n",
    "    # Transform the features\n",
    "    X_H_train_transform = poly.fit_transform(X_H_train)\n",
    "    X_H_test_transform = poly.fit_transform(X_H_test)\n",
    "\n",
    "    LR_Model_HOME_EG = LinearRegression()\n",
    "    # Fit the model using training data\n",
    "    LR_Model_HOME_EG.fit(X_H_train_transform, y_H_train)\n",
    "    # Make predictions using the model we have created\n",
    "    LR_H_predictions_test = LR_Model_HOME_EG.predict(X_H_test_transform)\n",
    "#     # Check the mean square error(MSE) for HomeTeam Expected Goals\n",
    "#     print(i, mean_squared_error(LR_H_predictions_test, y_H_test))\n",
    "\n",
    "    # Transform the features\n",
    "    X_A_train_transform = poly.fit_transform(X_A_train)\n",
    "    X_A_test_transform = poly.fit_transform(X_A_test)\n",
    "\n",
    "    LR_Model_AWAY_EG = LinearRegression()\n",
    "    # Fit the model using training data\n",
    "    LR_Model_AWAY_EG.fit(X_A_train_transform, y_A_train)\n",
    "    # Make predictions using the model we have created\n",
    "    LR_A_predictions_test = LR_Model_AWAY_EG.predict(X_A_test_transform)\n",
    "#     # Check the mean square error(MSE) for AwayTeam Expected Goals\n",
    "#     print(i, mean_squared_error(LR_A_predictions_test, y_A_test))\n",
    "\n",
    "    curr_mse_home = mean_squared_error(LR_H_predictions_test, y_H_test)\n",
    "    curr_mse_away = mean_squared_error(LR_A_predictions_test, y_A_test)\n",
    "\n",
    "    if curr_mse_home < min_mse_home:\n",
    "        best_poly1 = poly\n",
    "        best_model_home = LR_Model_HOME_EG\n",
    "        min_mse_home = curr_mse_home\n",
    "\n",
    "    if curr_mse_away < min_mse_away:\n",
    "        best_poly2 = poly\n",
    "        best_model_away = LR_Model_AWAY_EG\n",
    "        min_mse_away = curr_mse_away\n",
    "        \n",
    "        \n",
    "# PART 2 - ADD EXPECTED GOALS:\n",
    "\n",
    "# Using the two regression classfiers above, predict the number of goals that the Home and Away teams will score for each row in the dataframe:\n",
    "HomeExGoals = []\n",
    "AwayExGoals = []\n",
    "\n",
    "# For each row, predict the home and away expected goals\n",
    "for index, row in df_epl.iterrows():\n",
    "    X_Home_features = np.array([[row[\"Day\"],row[\"Month\"],row[\"HomeTeam_Enc\"],row[\"FTHG_AVG\"],row[\"HTHG_AVG\"],row[\"HS_AVG\"]]])\n",
    "    X_Away_features = np.array([[row[\"Day\"],row[\"Month\"],row[\"AwayTeam_Enc\"],row[\"FTAG_AVG\"],row[\"HTAG_AVG\"],row[\"AS_AVG\"]]])\n",
    "    # Transform features since we use polynomial regression\n",
    "    X_Home_features_transform = best_poly1.fit_transform(X_Home_features)\n",
    "    X_Away_features_transform = best_poly2.fit_transform(X_Away_features)\n",
    "    # Use the best polynomial classifier - Note the prediction is a 1 by 1 vector\n",
    "    ex_home_goals = best_model_home.predict(X_Home_features_transform)[0]\n",
    "    ex_away_goals = best_model_away.predict(X_Away_features_transform)[0]\n",
    "    # Add prediciton to list\n",
    "    HomeExGoals.append(ex_home_goals)\n",
    "    AwayExGoals.append(ex_away_goals)\n",
    "\n",
    "# Add this data into the dataframe\n",
    "df_epl[\"Ex_Goals_Home\"] = HomeExGoals\n",
    "df_epl[\"Ex_Goals_Away\"] = AwayExGoals\n",
    "\n",
    "\n",
    "# # Turn the catergorical data into labels using same method from before\n",
    "# df_epl[\"AwayTeam_Enc\"] = df_epl[\"AwayTeam\"].astype(\"category\").cat.codes\n",
    "# df_epl[\"HomeTeam_Enc\"] = df_epl[\"HomeTeam\"].astype(\"category\").cat.codes\n",
    "# df_epl = df_epl.drop(['HomeTeam', 'AwayTeam', 'Div'], axis=1)\n",
    "# # Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "# df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"])\n",
    "# df_epl[\"Day\"] = df_epl[\"Date\"].dt.day\n",
    "# df_epl[\"Month\"] = df_epl[\"Date\"].dt.month \n",
    "# df_epl[\"Year\"] = df_epl[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Removing Pre-encoded Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_epl.drop(['FTR','HTR','Referee','Home_Manager','Away_Manager'],inplace=True,axis=1)\n",
    "df_epl.drop(['HTR','Referee','Home_Manager','Away_Manager'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Breakdown of Features In The Dataframe/Dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Final Dataframe containing all features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Feature Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Manual Feature Selection & Motivations](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_1_1_'></a>[Preparation](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every feature set the dictionary `featuresets_dict` contains a *set_maker* callable and a *features_list* list. The set maker applied to the feature_list will output a design matrix for the corresponding feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict: dict[str, tuple[Callable, list[Any]]] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrix_selection(features_list=[]) -> np.ndarray:\n",
    "    df_epl_final2 = df_epl.copy().loc[:,features_list].values\n",
    "    return df_epl_final2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dimensionality reduction algorithms, we need to use a wrapper, because of the variable *set_maker* function. This effectively makes the wrapper a higher-order function returning a *set_maker*, given appropriate dimensionality reduction transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrix_pca_wrapper(dim_red_model_function: Callable) -> Callable:\n",
    "    return lambda features_list: dim_red_model_function(\n",
    "            df_epl.copy().loc[:,features_list].values\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrix_ae_wrapper(dim_red_model_function: Callable) -> Callable:\n",
    "    return lambda features_list: dim_red_model_function(\n",
    "        df_epl.copy().loc[:,features_list].values\n",
    "    ).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_2_'></a>[Introduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our design matrix by extracting the relevant feature columns we require using the .loc method. We also create the output y vector by extracting the encoded FTR values. Using the matrix and vector, we experiment with different classifiers to see how they perform. In the results section we compare the performance of each classifier and provide brief explanations of why the result might be what they are and how we aim to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_3_'></a>[FEATURE SET 1 - Baseline Classifier: Random Guesses](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_3_1_'></a>[Motivation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom random guess classifier implementation below. To ensure convenience it is compatibile with some sklearn functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_3_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set1'] = (create_design_matrix_selection, ['HomeTeam_Enc', 'AwayTeam_Enc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_design_matrix_manual_selection(df, ):\n",
    "#   df_epl_final2 = df_epl.loc[:,['HomeTeam_Enc', 'AwayTeam_Enc']].values\n",
    "#   return df_epl_final2\n",
    "\n",
    "# def create_df_copy(df_epl):\n",
    "#   return df_epl.copy()\n",
    "\n",
    "# df_final = create_df(df_epl)\n",
    "# X = create_design_matrix(df_final)\n",
    "# y = df_final.loc[:,['FTR']].values.ravel()\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# FTR_encoder = LabelEncoder()\n",
    "# y_train = FTR_encoder.fit_transform(y_train).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_3_3_'></a>[Evaluate without cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit random guesser\n",
    "# clf = fit_rg_n_cv(X_train, y_train)\n",
    "# clf_eval(clf, X_test, y_test, featureset_name=\"Feature Set 1\", classifier_name=\"RG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_4_'></a>[FEATURE SET 2 - Simple: (HomeTeam, AwayTeam and Month)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_4_1_'></a>[Motivation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY -> Firstly, we trained models only using the home team, away team and month as feature set. We evaluated and compared these models. Since these models have not lead to great accuracies, we did not perform model selection on them. As the models become more complex (and include more features) we start to include the results from model selection.\n",
    "\n",
    "(NOTE: We have also tried variations of this MODEL - with other basic features like day and referee added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_4_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set2'] = (\n",
    "    create_design_matrix_selection,\n",
    "    [\n",
    "        'HomeTeam_Enc',\n",
    "        'AwayTeam_Enc', \n",
    "        'Month'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_5_'></a>[FEATURE SET 3 - Included Averages (All Seasons + Recent Season)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_5_1_'></a>[Motivation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we aim to improve our previous score by adding some extra features. We use the same features from the previous section: date, HomeTeam and AwayTeam, BUT now we also add some averages of past stats for the two specific teams in question. These past average stats are obtained by filtering the df_epl dataframe to get two filtered dataframes. One will include the matches played by HomeTeam at Home against ALL other teams in that specific season. The other will include the matches played by AwayTeam Away against ALL other teams in that specific season. Similarly is done by condering at all previous games instead of only the past games played in recent season. Given these filtered dataframes we take an average of the columns (with stats) like HR, AR, etc. This will provide us with the past stats for games played between the two teams against all other teams in that specific season (and over all seasons). We can then use these past average stats as features to input into the classifier. This should produce an improvement because recent/all season average data should help provide a good estimation of team performance.current season games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_5_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set3'] = (\n",
    "    create_design_matrix_selection, \n",
    "    [\n",
    "        'Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc',\n",
    "        'HS_HISTORY','AS_HISTORY','HST_HISTORY','AST_HISTORY',\n",
    "        'HF_HISTORY','AF_HISTORY','HC_HISTORY','AC_HISTORY',\n",
    "        'HY_HISTORY','AY_HISTORY','HR_HISTORY','AR_HISTORY',\n",
    "        'HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG',\n",
    "        'HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_6_'></a>[FEATURE SET 4 – Complex: Included Expected Goals, Number of Past Wins (Home & Away)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_6_1_'></a>[Motivation](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY -> Here we aim to improve our previous score by adding some complex features to the design matrix. These features are Home/Away Team ratings, the expected/predicted goals for each team and the number of past wins in season for the home/away team.\n",
    "\n",
    "The Home/Away team ratings are calculated by using a weighted sum of some of the averages we calculated in MODEL 4. The expected goals for each team are calculated using a polynomial regression classifier that is trained using again some of the features from the previous MODELS. The number of past wins are calculated by summing the number of wins by the team in the season. Once we calculate these extra features, we can then use them as extra inputs into the classifier.\n",
    "\n",
    "NOTE: We must remove the rows in the dataframe where the average values of stats/features are 'nan'; we get these values because either its the teams first game of the season (hence no past data) OR because we have not got the past stats for these matches in the dataset. We cannot use the 'nan',values for the classifier training and hence have to remove these rows. We can then train a classifier using this final dataframe.\n",
    "\n",
    "SO, In the final model/classifier, in the case where we DO NOT have these past stats of the teams playing, we need to switch back to using a variation of MODEL 2 which takes the 4 basic features: day, month, HomeTeam and AwayTeam. In the case where we DO have these past stats for the teams, we can use this model/classifier and input the features like HST_AVG. We would find these by using the filter_dataframe() and average_columns() functions to find them for any two specific teams playing each other on some date.\n",
    "\n",
    "NOTE: This applies to MODEL 4 as well. A similar case applies to MODEL 3 BUT in this case it happens when the two teams playing each other have not played each other before OR when we do not have this data in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_6_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set4'] = (\n",
    "    create_design_matrix_selection, \n",
    "    [\n",
    "        'Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc',\n",
    "        'HS_HISTORY','AS_HISTORY','HST_HISTORY','AST_HISTORY',\n",
    "        'HF_HISTORY','AF_HISTORY','HC_HISTORY','AC_HISTORY',\n",
    "        'HY_HISTORY','AY_HISTORY','HR_HISTORY','AR_HISTORY',\n",
    "        'HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG',\n",
    "        'HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG',\n",
    "        'HW_AVG','AW_AVG','Ex_Goals_Home','Ex_Goals_Away'\n",
    "    ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_7_'></a>[FEATURE SET 5 – Complex: Included Team Managers and Previous Season Standings](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_7_1_'></a>[Motivation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY -> Here we aim to improve our previous score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_1_7_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set5'] = (\n",
    "    create_design_matrix_selection, \n",
    "    [\n",
    "        'Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc',\n",
    "        'HS_HISTORY','AS_HISTORY','HST_HISTORY','AST_HISTORY',\n",
    "        'HF_HISTORY','AF_HISTORY','HC_HISTORY','AC_HISTORY',\n",
    "        'HY_HISTORY','AY_HISTORY','HR_HISTORY','AR_HISTORY',\n",
    "        'HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG',\n",
    "        'HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG',\n",
    "        'HW_AVG','AW_AVG','Ex_Goals_Home','Ex_Goals_Away',\n",
    "        'HomeStanding_PrevSeason','AwayStanding_PrevSeason',\n",
    "        'DiffStanding_PrevSeason','Home_Manager_Enc','Away_Manager_Enc'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Automatic Feature Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Choose K Best Features](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_1_'></a>[Auto Select Features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl2 = df_epl.copy()\n",
    "#print(df_epl2.columns)\n",
    "\n",
    "X = df_epl2.drop(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG','HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY','HR', 'AR','FTR_Enc','HTR_Enc'], axis=1)  #independent columns\n",
    "y = df_epl2.loc[:,\"FTR_Enc\"]\n",
    "#apply SelectKBest class to extract top X best features\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Features','Score']  #naming the dataframe columns\n",
    "\n",
    "featureScores = featureScores[featureScores['Score'] > 0.0]\n",
    "\n",
    "# print(featureScores.colums.size)\n",
    "\n",
    "# print(featureScores.nlargest(50,'Score'))\n",
    "\n",
    "# num_of_features = 20\n",
    "# ordered_features = featureScores.nlargest(num_of_features,'Score')\n",
    "\n",
    "# selected_features = list(ordered_features['Features'])\n",
    "selected_features = list(featureScores['Features'])\n",
    "\n",
    "# add selectd features for later use\n",
    "#seleced_features_auto['Choose_K_Best'] = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set_skb'] = (\n",
    "    create_design_matrix_selection, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Recursive Feature Elimination](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_2_1_'></a>[Auto Select Features - NOTE: EXTREMELY SLOW - Careful when running this cell](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl2 = df_epl.copy()\n",
    "\n",
    "X = df_epl2.drop(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG','HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY','HR', 'AR','FTR_Enc','HTR_Enc'], axis=1)  #independent columns\n",
    "y = df_epl2.loc[:,\"FTR_Enc\"]\n",
    "\n",
    "\n",
    "# Try using Linear SVM to select important features\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\")\n",
    "# The \"accuracy\" scoring shows the proportion of correct classifications\n",
    "#rf = RandomForestClassifier()\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(\n",
    "    estimator=svc,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=scoring[refit],\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=min_features_to_select,\n",
    ")\n",
    "rfecv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_2_2_'></a>[Evaluation on CV results of RFE method](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (balanced accuracy)\")\n",
    "plt.plot(\n",
    "    range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),\n",
    "    rfecv.grid_scores_,\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfecv.feature_names_in_[rfecv.support_])\n",
    "print(len(rfecv.feature_names_in_[rfecv.support_]))\n",
    "selected_features = rfecv.feature_names_in_[rfecv.support_]\n",
    "\n",
    "# add selectd features for later use\n",
    "seleced_features_auto['RFE'] = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_2_3_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set_rfe'] = (\n",
    "    create_design_matrix_selection, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_3_'></a>[Using Select From Model (L1-based feature selection)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_3_1_'></a>[Auto Select Features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl2 = df_epl.copy()\n",
    "#print(df_epl2.columns)\n",
    "\n",
    "X = df_epl2.drop(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG','HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY','HR', 'AR','FTR_Enc','HTR_Enc'], axis=1)  #independent columns\n",
    "y = df_epl2.loc[:,\"FTR_Enc\"]\n",
    "\n",
    "# Using a large nunmber here is because SVM doesn't converge if number of iteration is small\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", multi_class= \"ovr\", dual=False, max_iter=1200000).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.feature_names_in_[model.get_support()])\n",
    "print(len(model.feature_names_in_[model.get_support()]))\n",
    "selected_features = model.feature_names_in_[model.get_support()]\n",
    "\n",
    "# add selectd features for later use\n",
    "seleced_features_auto['SEL_Model_L1'] = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_3_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set_select_model_L1'] = (\n",
    "    create_design_matrix_selection, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_4_'></a>[Tree Based Model (select from model)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_4_1_'></a>[Auto Select Features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl2 = df_epl.copy()\n",
    "#print(df_epl2.columns)\n",
    "\n",
    "X = df_epl2.drop(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG','HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY','HR', 'AR','FTR_Enc','HTR_Enc'], axis=1)  #independent columns\n",
    "y = df_epl2.loc[:,\"FTR_Enc\"]\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.feature_names_in_[model.get_support()])\n",
    "print(len(model.feature_names_in_[model.get_support()]))\n",
    "selected_features = model.feature_names_in_[model.get_support()]\n",
    "\n",
    "# add selectd features for later use\n",
    "seleced_features_auto['SEL_Model_RF'] = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_4_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set_select_model_tree'] = (\n",
    "    create_design_matrix_selection, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_5_'></a>[Sequential Feature Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_5_1_'></a>[Auto Select Features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl2 = df_epl.copy()\n",
    "#print(df_epl2.columns)\n",
    "\n",
    "X = df_epl2.drop(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG','HTAG', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY','HR', 'AR','FTR_Enc','HTR_Enc'], axis=1)  #independent columns\n",
    "y = df_epl2.loc[:,\"FTR_Enc\"]\n",
    "\n",
    "#clf = RandomForestClassifier()\n",
    "#clf = clf.fit(X, y)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "sfs = SequentialFeatureSelector(estimator=clf,\n",
    "    n_features_to_select=\"auto\",\n",
    "    cv=cv,\n",
    "    scoring=scoring[refit],\n",
    "    n_jobs=-1,\n",
    "    )\n",
    "sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sfs.feature_names_in_[sfs.get_support()])\n",
    "print(len(sfs.feature_names_in_[sfs.get_support()]))\n",
    "selected_features = sfs.feature_names_in_[sfs.get_support()]\n",
    "\n",
    "# add selectd features for later use\n",
    "seleced_features_auto['SFS'] = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_5_2_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_dict['feature_set_sfs'] = (\n",
    "    create_design_matrix_selection, \n",
    "    selected_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Feature Extraction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Feature extraction using dimensionality reduction](#toc0_)\n",
    "\n",
    "In this section, we try to capture the most important features from the classification point of view, using linear (PCA) and non-linear (autoencoder) dimensionality reduction techniques. In the whole section we are using the richest dataset we have developed - Feature Set 5.\n",
    "\n",
    "To select the best parameters for both dimensionality reduction algorithms, we need to ensure that the latent space with lower number of dimensions can be used to reconstruct the original data at least is some sense. To do this, we will use the MSE reconstruction loss for both PCA and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set5']\n",
    "X = set_maker(features_list)\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing for faster convergence with autoencoders and comparison with PCA using the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[PCA](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's create a baseline PCA algoritm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually compare the reconstructed and original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.inverse_transform(pca.transform(X_test))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((pca.inverse_transform(pca.transform(X_test))[1] - X_test[1]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pca(X_train, X_test, n_components=[]):\n",
    "    val_loss = []\n",
    "    #for components in n_components:\n",
    "    for components in tqdm(n_components):\n",
    "        pca = PCA(n_components=components)\n",
    "        pca.fit(X_train)\n",
    "        X_proj = pca.transform(X_test)\n",
    "        X_recon = pca.inverse_transform(X_proj)\n",
    "        # MSE\n",
    "        val_loss.append(np.sum((X_test - X_recon) ** 2, axis=1).mean())\n",
    "    plt.scatter(n_components, val_loss)\n",
    "    plt.ylabel('reconstruction loss on a test set')\n",
    "    plt.xlabel('number of components used in PCA')\n",
    "    plt.title(\"Number of components vs reconstruction loss (MSE) using PCA\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the reconstruction loss vs the number of components used in PCA we can visualize whether the encoded data has enough expressive power to be used as a good feature extractor for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pca(X_train, X_test, n_components=range(1, X_train.shape[1]+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create datasets using PCA with 20 and 30 components that can be used later in cross-validation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature set with PCA 30, used later in CV\n",
    "PCA30 = PCA(n_components=30)\n",
    "PCA30.fit(X_train)\n",
    "\n",
    "PCA20 = PCA(n_components=20)\n",
    "PCA20.fit(X_train)\n",
    "\n",
    "featuresets_dict['feature_set_PCA20'] = (\n",
    "    create_design_matrix_pca_wrapper(PCA20.transform),\n",
    "    featuresets_dict['feature_set5'][1] # feature list\n",
    ")\n",
    "\n",
    "featuresets_dict['feature_set_PCA30'] = (\n",
    "    create_design_matrix_pca_wrapper(PCA30.transform),\n",
    "    featuresets_dict['feature_set5'][1] # feature list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[Autoencoder](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(32, activation='relu'),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(32, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(X_train.shape[1], activation='relu'),\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(X_train, X_test, latent_dim, epochs, verbose=2):\n",
    "    # NOTE: reset seed, everytime before training\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    tf.keras.utils.set_random_seed(42)   \n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    autoencoder = Autoencoder(latent_dim) \n",
    "    autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                verbose=verbose)\n",
    "    return autoencoder\n",
    "\n",
    "def evaluate_autoencoders(X_train, X_test, latent_dims=[], epochs_lst=[50]):\n",
    "    for i, latent_dim in enumerate(latent_dims):\n",
    "        print(f\"Running latent_space of size {latent_dim}\")\n",
    "        for epochs in tqdm(epochs_lst):\n",
    "            autoencoder = train_autoencoder(X_train, X_test, latent_dim, epochs, verbose=0)\n",
    "            loss = autoencoder.history.history['loss']\n",
    "            val_loss = autoencoder.history.history['val_loss']\n",
    "            plt.plot(range(epochs), val_loss, label=f\"latent space of size {latent_dim}\")\n",
    "\n",
    "    plt.ylabel(\"Reconstruction loss (MSE)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Autoencoder reconstruction loss (MSE) on different latent space sizes (50 epochs)\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_autoencoders_latent_space(X_train, X_test, epochs=20, latent_dims=[]):\n",
    "    val_losses = []\n",
    "    for latent_dim in tqdm(latent_dims):\n",
    "        autoencoder = train_autoencoder(X_train, X_test, latent_dim, epochs, verbose=0)\n",
    "        val_losses.append(autoencoder.evaluate(X_test, X_test))\n",
    "\n",
    "    plt.scatter(latent_dims, val_losses)\n",
    "    plt.ylabel('Reconstruction loss (MSE)')\n",
    "    plt.xlabel('size of the latent space (neurons)')\n",
    "    plt.title(\"Size of the latent space vs reconstruction loss (MSE) using PCA\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline autoencoder with latent space of size 10, trained in 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_autoencoder = train_autoencoder(X_train, X_test, latent_dim=10, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a reconstructed vs real data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_autoencoder.decoder(baseline_autoencoder.encoder(X_train).numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoders(X_train, X_test, latent_dims=[2, 4, 8, 16, 24, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoders_latent_space(X_train, X_test, latent_dims=range(2, 31), epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate latent space encodings of the X_train and X_test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one/two? a feature sets with latent space size = 16 and 30\n",
    "AE4 = train_autoencoder(X_train, X_test, latent_dim=4, epochs=30)\n",
    "AE16 = train_autoencoder(X_train, X_test, latent_dim=16, epochs=30)\n",
    "AE30 = train_autoencoder(X_train, X_test, latent_dim=30, epochs=30)\n",
    "\n",
    "featuresets_dict['feature_set_AE4'] = (\n",
    "    create_design_matrix_pca_wrapper(AE4.encoder),\n",
    "    featuresets_dict['feature_set5'][1] # feature list\n",
    ")\n",
    "\n",
    "featuresets_dict['feature_set_AE16'] = (\n",
    "    create_design_matrix_pca_wrapper(AE16.encoder),\n",
    "    featuresets_dict['feature_set5'][1] # feature list\n",
    ")\n",
    "\n",
    "featuresets_dict['feature_set_AE30'] = (\n",
    "    create_design_matrix_pca_wrapper(AE30.encoder),\n",
    "    featuresets_dict['feature_set5'][1] # feature list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Methodology Overview](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[Auxiliary Functions + Classifier interfaces](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_1_'></a>[Evaluation helpers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_report(y_pred, y_test):\n",
    "  y_pred = y_pred.ravel()\n",
    "  y_test = y_test.ravel()\n",
    "\n",
    "  print(\"Balanced Accuracy: \", balanced_accuracy_score(y_test,y_pred))\n",
    "  print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "  # handle f1 score zero division\n",
    "  # https://stackoverflow.com/questions/62326735/metrics-f1-warning-zero-division\n",
    "  print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "  ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for summary part of each feature Set\n",
    "model_acc_dict = {\n",
    "  'RG': 0,\n",
    "  'DT': 0,\n",
    "  'RF': 0,\n",
    "  'KNN': 0,\n",
    "  'SVM': 0,\n",
    "  'XGB': 0,\n",
    "  'NN': 0\n",
    "}\n",
    "\n",
    "compare_feature_sets_dict = {\n",
    "    fs_name: {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0} \n",
    "    for fs_name in featuresets_dict.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_2_'></a>[Plotting helpers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 1\", fig_size=(9, 9)):\n",
    "  model_names = list(model_acc_dict.keys())\n",
    "  '''\n",
    "  model_names = [\"Random Guess\", \"Decision Tree\", \n",
    "                  \"Random Forest\", \"K Nearest Neighbors\",\n",
    "                  \"Support Vector Machine\", \"XGB\",\n",
    "                  \"Nerual Network\"]\n",
    "  '''\n",
    "  \n",
    "  x_label = \"Balanced Accuracy (%)\"\n",
    "  y_label = \"Models trained\"\n",
    "\n",
    "  accs = model_acc_dict.values()\n",
    "\n",
    "  fig = plt.figure(figsize=fig_size)\n",
    "  ax = fig.gca()\n",
    "  p1 = ax.barh(model_names, accs)\n",
    "\n",
    "  ax.set_title(title, fontsize=12)\n",
    "  ax.set_xlabel(x_label)\n",
    "  ax.set_ylabel(y_label)\n",
    "\n",
    "  for i, v in enumerate(accs):\n",
    "      ax.text(v//2, i, str(v), color='white', fontsize=9, ha='left', va='center')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used to plot changes on training loss & cross validation loss\n",
    "'''\n",
    "def plot_train_test_acc(results , scoring, param_x = \"param_max_depth\",title=\"GridSearchCV evaluation\", xlabel=\"max_depth\", ylabel=\"Score\",xlim=(0,100), ylim=(0.4,1), fig_size=(9, 9)):\n",
    "\n",
    "    # REF:https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(title, fontsize=16)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results[param_x].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
    "        for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "            sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
    "            sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
    "            ax.fill_between(\n",
    "                X_axis,\n",
    "                sample_score_mean - sample_score_std,\n",
    "                sample_score_mean + sample_score_std,\n",
    "                alpha=0.1 if sample == \"test\" else 0,\n",
    "                color=color,\n",
    "            )\n",
    "            # change label(test) -> cross validation to avoid confusion\n",
    "            if sample == \"test\":\n",
    "              sample = \"cross validation\"\n",
    "            ax.plot(\n",
    "                X_axis,\n",
    "                sample_score_mean,\n",
    "                style,\n",
    "                color=color,\n",
    "                alpha=1 if sample == \"cross validation\" else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample),\n",
    "            )\n",
    "\n",
    "        best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "        best_score = results[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot(\n",
    "            [\n",
    "                X_axis[best_index],\n",
    "            ]\n",
    "            * 2,\n",
    "            [0, best_score],\n",
    "            linestyle=\"-.\",\n",
    "            color=color,\n",
    "            marker=\"x\",\n",
    "            markeredgewidth=3,\n",
    "            ms=8,\n",
    "        )\n",
    "    \n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.4f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_3_'></a>[Metrics and classifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc8_1_3_1_'></a>[scoring metrics and define cross validation data split](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: Scoring metrics\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\"\"\"\n",
    "scoring = {\"Accuracy\": \"accuracy\", \"Balanced_accuracy\": \"balanced_accuracy\"}\n",
    "refit = \"Balanced_accuracy\"\n",
    "#cv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_1_1_'></a>[helper function for producing report](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval(clf, X_test, y_test, featureset_name, classifier_name):\n",
    "    # Make predictions using the model we have created\n",
    "    y_pred = clf.predict(X_test).ravel()\n",
    "    # Reconverting prediction values (i.e. 0, 1 or 2) back into (H, D or A) using the FTR_encoder defined in earlier cell\n",
    "    y_pred = FTR_encoder.inverse_transform(y_pred)\n",
    "    y_test = y_test.ravel()\n",
    "    y_test = FTR_encoder.inverse_transform(y_test)\n",
    "\n",
    "    evaluate_report(y_pred, y_test)\n",
    "    model_acc_dict[classifier_name] = round(balanced_accuracy_score(y_test,y_pred)*100, 2)\n",
    "    # NOTE: we are not logging values for random guesser here.\n",
    "    compare_feature_sets_dict[featureset_name][classifier_name] = round(balanced_accuracy_score(y_test,y_pred)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc8_1_3_2_'></a>[Classifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_1_'></a>[Random Guesses](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGuessClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Custom implementation of the random guess classifier. \n",
    "    \n",
    "    Compatible with sklearn.metric.plot_confusion_matrix function.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._labels: np.ndarray = None\n",
    "\n",
    "    def fit(self, x_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"Does not use x_train at all (who would actually want to use it lol). \n",
    "        Saves available labels from y_train.\n",
    "        \"\"\"\n",
    "        self._labels = np.unique(y_train)\n",
    "\n",
    "    def predict(self, x_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"For every sample in x_test, chooses a label from self._labels at random\"\"\"\n",
    "        np.random.seed(42)\n",
    "        return np.array(list(map(lambda _: np.random.choice(self._labels, 1), x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rg_n_cv(X_train, y_train):\n",
    "    rgc = RandomGuessClassifier()\n",
    "    rgc.fit(X_train, y_train)\n",
    "    return rgc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_2_'></a>[Decision Tree Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dt_n_cv(X_train, y_train):\n",
    "    # Create an empty Tree model\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    # Fit the model using training data\n",
    "    dt.fit(X_train, y_train)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decision Tree\"\"\"\n",
    "def fit_DT(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'ccp_alpha': 0.0,\n",
    "    'class_weight': None,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'random_state': None,\n",
    "    'splitter': 'best'}\n",
    "    \"\"\"\n",
    " \n",
    "    classifier = DecisionTreeClassifier(random_state=42)\n",
    "    print(list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)))\n",
    "    param_grid = {'max_depth': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)),\n",
    "                  'max_features': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_3_'></a>[Random Forest Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_n_cv(X_train, y_train):\n",
    "    # Create an empty Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    # Fit the model using training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_RF(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'bootstrap': True,\n",
    "    'ccp_alpha': 0.0,\n",
    "    'class_weight': None,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,\n",
    "    'max_features': 'sqrt',\n",
    "    'max_leaf_nodes': None,\n",
    "    'max_samples': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': None,\n",
    "    'oob_score': False,\n",
    "    'random_state': None,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False}\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {'max_depth':list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)),\n",
    "                  'max_features': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_4_'></a>[K-Nearest Neighbours (KNN) Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn_n_cv(X_train, y_train):\n",
    "    # Create an empty KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "    # Fit the model using training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_KNN(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'algorithm': 'auto',\n",
    "    'leaf_size': 30,\n",
    "    'metric': 'minkowski',\n",
    "    'metric_params': None,\n",
    "    'n_jobs': None,\n",
    "    'n_neighbors': 5,\n",
    "    'p': 2,\n",
    "    'weights': 'uniform'}\n",
    "    \"\"\"\n",
    "    classifier = KNeighborsClassifier()\n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {'n_neighbors':list(np.linspace(start=3, stop=100, num=50, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_5_'></a>[Support Vector Machine (SVM) Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm_n_cv(X_train, y_train):\n",
    "    # Create an empty svm classifier model with RBF Kernal\n",
    "    svc = svm.SVC(kernel='rbf')\n",
    "    # Fit the model using training data\n",
    "    svc.fit(X_train, y_train)\n",
    "    return svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_SVM(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'C': 1.0,\n",
    "    'break_ties': False,\n",
    "    'cache_size': 200,\n",
    "    'class_weight': None,\n",
    "    'coef0': 0.0,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'degree': 3,\n",
    "    'gamma': 'scale',\n",
    "    'kernel': 'rbf',\n",
    "    'max_iter': -1,\n",
    "    'probability': False,\n",
    "    'random_state': None,\n",
    "    'shrinking': True,\n",
    "    'tol': 0.001,\n",
    "    'verbose': False}\n",
    "    \"\"\"\n",
    "    classifier = svm.SVC()\n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {'C':[1.0,5.0,10.0,15.0, 20.0, 25.0, 30.0]\n",
    "                  }\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_6_'></a>[XGB](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_n_cv(X_train, y_train):\n",
    "    # create a baseline XGBoost classifier\n",
    "    # changed use_label_encoder=False here \n",
    "\n",
    "    ## NOTE: USE CPU\n",
    "    #xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "\n",
    "    ## NOTE: USE GPU\n",
    "    xgb = XGBClassifier(eval_metric=\"mlogloss\", tree_method=\"gpu_hist\", gpu_id=0)\n",
    "\n",
    "    # Fit the model using training data\n",
    "    xgb.fit(X_train, y_train)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_XGB(X, y):\n",
    "\n",
    "    ## NOTE: USE CPU\n",
    "    #classifier = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "\n",
    "    ## NOTE: USE GPU\n",
    "    classifier = XGBClassifier(eval_metric=\"mlogloss\", tree_method=\"gpu_hist\", gpu_id=0)\n",
    "    \n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {\n",
    "        \"booster\": ['gbtree', 'dart'],\n",
    "        \"learning_rate\": [0.1, 0.3, 0.5],\n",
    "        \"n_estimators\": [5, 10, 20, 50, 100],\n",
    "        \"max_depth\": [5, 10, 20, 50, 100],\n",
    "        \"tree_method\": ['exact', 'approx', 'hist'],\n",
    "        \"eval_metric\": ['mlogloss']\n",
    "    }\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_7_'></a>[Neural Network](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(input_size):\n",
    "\n",
    "    # Prevent OOM\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_size, activation='relu', input_dim=input_size),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'],\n",
    "                weighted_metrics=['accuracy']\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_n_cv(X_train, y_train):\n",
    "    # warp this obj into a sklearn classifier\n",
    "    model_warpped = KerasClassifier(model=build_clf, input_size=X_train.shape[1], epochs=20, verbose=1)\n",
    "    #print(model_warpped.get_params()) \n",
    "\n",
    "    # balanced accuracy weights\n",
    "    counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = dict(zip(counts[0], np.reciprocal(counts[1].astype('float64'))))\n",
    "\n",
    "    # NOTE: SEEDING. reset seed, everytime before training\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    tf.keras.utils.set_random_seed(42)   \n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    model_warpped.fit(X_train, y_train, class_weight=class_weights)\n",
    "\n",
    "    return model_warpped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_NN(X_train, y_train):\n",
    "    '''\n",
    "    {'model': <function build_clf at 0x7fad83129870>, \n",
    "    'build_fn': None, 'warm_start': False, \n",
    "    'random_state': None, \n",
    "    'optimizer': 'rmsprop', \n",
    "    'loss': None, \n",
    "    'metrics': None, \n",
    "    'batch_size': None, \n",
    "    'validation_batch_size': None, \n",
    "    'verbose': 1, \n",
    "    'callbacks': None, \n",
    "    'validation_split': 0.0, \n",
    "    'shuffle': True, \n",
    "    'run_eagerly': False, \n",
    "    'epochs': 10, \n",
    "    'input_size': 28, \n",
    "    'class_weight': None}\n",
    "    '''\n",
    "\n",
    "    # warp this obj into a sklearn classifier\n",
    "    model_warpped = KerasClassifier(model=build_clf, input_size=X_train.shape[1], epochs=10)\n",
    "    print(model_warpped.get_params()) \n",
    "\n",
    "    # balanced accuracy weights\n",
    "    counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = dict(zip(counts[0], np.reciprocal(counts[1].astype('float64'))))\n",
    "\n",
    "    param_grid = {\n",
    "        'epochs': [1, 5, 10, 20, 30],\n",
    "        'optimizer': ['rmsprop', 'adam', 'adagrad'],\n",
    "        'batch_size': [8, 16, 64],\n",
    "        'class_weight': [class_weights]\n",
    "    }\n",
    "\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        model_warpped, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        # For reproduce result. Change n_jobs to None or 1\n",
    "        n_jobs=-2, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "\n",
    "    # NOTE: SEEDING. reset seed, everytime before training\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    tf.keras.utils.set_random_seed(42)   \n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    clf = grid.fit(X_train, y_train)\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc8_1_3_2_8_'></a>[Feature set interfaces](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set2\"):\n",
    "\n",
    "    # fit decision Tree Classifier\n",
    "    clf = fit_dt_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"DT\")\n",
    "    \n",
    "    # fit random forest\n",
    "    clf = fit_rf_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"RF\")\n",
    "\n",
    "    # fit knn\n",
    "    clf = fit_knn_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"KNN\")\n",
    "\n",
    "    # fit SVM\n",
    "    clf = fit_svm_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"SVM\")   \n",
    "\n",
    "    # fit XGB\n",
    "    clf = fit_xgb_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"XGB\")  \n",
    "\n",
    "    # fit NN\n",
    "    clf = fit_nn_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"NN\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set2\"):\n",
    "    \n",
    "    # fit decision Tree Classifier\n",
    "    clf = fit_DT(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring, title=\"Evaluation using Cross Validation (Decision Tree)\", xlabel=\"max_depth\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,20), ylim=(0.2,1.0))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"DT\")\n",
    "\n",
    "    # fit Random Forest Classifier\n",
    "    clf = fit_RF(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring, title=\"Evaluation using Cross Validation (Random Forest)\", xlabel=\"max_depth\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,20), ylim=(0.2,1.0))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"RF\")\n",
    "\n",
    "    # fit KNN\n",
    "    clf = fit_KNN(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_n_neighbors\", title=\"Evaluation using Cross Validation (KNN)\", xlabel=\"number of neighbors\", ylabel=\"Score\", fig_size=(8,8), xlim=(3,100), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"KNN\")\n",
    "\n",
    "    # fit SVM\n",
    "    clf = fit_SVM(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_C\", title=\"Evaluation using Cross Validation (SVM, rbf kernel)\", xlabel=\"C value\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,35), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"SVM\")\n",
    "\n",
    "    # XGB\n",
    "    clf = fit_XGB(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    #plot validation loss vs trainning loss\n",
    "    #plot_train_test_acc()\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"XGB\")\n",
    "\n",
    "    # NN\n",
    "    clf = fit_NN(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_epochs\", title=\"Evaluation using Cross Validation (NN)\", xlabel=\"Number of epoch\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,35), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"NN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Model Training & Validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_1_'></a>[Manual Feature Sets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_1_'></a>[Feature Set 1](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_1_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set1']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_1_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random guesser\n",
    "clf = fit_rg_n_cv(X_train, y_train)\n",
    "clf_eval(clf, X_test, y_test, featureset_name=\"feature_set1\", classifier_name=\"RG\")\n",
    "\n",
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_1_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 1\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_2_'></a>[Feature Set 2](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_2_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set2']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_2_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_2_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 2\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_3_'></a>[Feature Set 3](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_3_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set3']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_3_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_3_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 3\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_4_'></a>[Feature Set 4](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_4_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set4']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_4_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_4_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 4\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_1_5_'></a>[Feature Set 5](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_5_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set5']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_5_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_5_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 5\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_5_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_1_5_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 5(CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_2_'></a>[Automatically Selected Feature Sets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_1_'></a>[Choose K Best Features](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_1_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set_skb']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_1_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_skb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_1_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Choose K Best Features\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_1_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_skb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_1_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Choose K Best Features(CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_2_'></a>[Recursive Feature Elimination](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_2_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set_rfe']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_2_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_rfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_2_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Recursive Feature Elimination\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_2_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_rfe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_2_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Recursive Feature Elimination(CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_3_'></a>[Using Select From Model (L1-based feature selection)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_3_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set_select_model_L1']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_3_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_select_model_L1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_3_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - SelectFromModel (L1)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_3_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_select_model_L1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_3_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - SelectFromModel (L1) (CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_4_'></a>[Tree Based Model (select from model)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_4_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set_select_model_tree']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_4_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_select_model_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_4_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Tree Based Model (select from model)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_4_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_select_model_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_4_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Tree Based Model (select from model) (CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc9_2_5_'></a>[Sequential Feature Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_5_1_'></a>[Create design matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_maker, features_list = featuresets_dict['feature_set_sfs']\n",
    "X = set_maker(features_list)\n",
    "\n",
    "y = df_epl.loc[:,['FTR_Enc']].values.ravel()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_5_2_'></a>[Evaluate without cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_sfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_5_3_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Sequential Feature Selection\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_5_4_'></a>[Evaluate with cross validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"feature_set_sfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc9_2_5_5_'></a>[Summary (CV)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Sequential Feature Selection (CV)\", fig_size=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc9_3_'></a>[Feature Extracted Feature Sets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Results](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Final Predictions on Test Set](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Conclusions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP0036_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "929de6c31a01da29cb4e64f921de52b26b0f92466a5b3a313ad7d1ab7783989a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
