{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.join(os.getcwd(), 'Group Coursework Brief-20221106', 'Data_Files', 'Data_Files')\n",
    "dirName_trainData = os.path.join(cwd, 'epl-full-training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG',\\n       'HTAG', 'HTR', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC',\\n       'AC', 'HY', 'AY', 'HR', 'AR'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-99baa85faee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HomeTeam\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AwayTeam\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"FTHG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FTAG\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"FTR\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HTHG\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HTAG\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HTR\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Referee\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HS\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AS\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HST\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AST\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HF\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AF\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HY\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AY\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"HR\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"AR\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_epl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_epl_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf_epl_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_epl_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    871\u001b[0m                     \u001b[1;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    748\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m             \u001b[1;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1039\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG',\\n       'HTAG', 'HTR', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC',\\n       'AC', 'HY', 'AY', 'HR', 'AR'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df_epl_train = pd.read_csv(dirName_trainData)\n",
    "cols = [\"Div\",\"Date\",\"HomeTeam\",\"AwayTeam\",\"FTHG\", \"FTAG\",\"FTR\",\"HTHG\",\"HTAG\",\"HTR\",\"Referee\",\"HS\",\"AS\", \"HST\",\"AST\",\"HF\",\"AF\",\"HC\",\"AC\",\"HY\",\"AY\",\"HR\",\"AR\"]\n",
    "\n",
    "df_epl_train = df_epl_train.loc[:, cols]\n",
    "df_epl_train = df_epl_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df_epl_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column from strings into datetime objects\n",
    "df_epl_train[\"Date\"] = pd.to_datetime(df_epl_train[\"Date\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_epl_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIM: A function that takes as input the date, HomeTeam and AwayTeam. It will filter the df_epl_train dataframe for matches between HomeTeam and AwayTeam that took place before the input date. Then take an average of the \n",
    "columns like HR, AR, etc. This will provide us with the past stats for games played in past between the two teams. We can then use these past stats (between the two teams) as features to input into the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the dataframe to include only matches where date is less than date specified and also only include matches where HomeTeam=input(HomeTeam) and AwayTeam=input(AwayTeam):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take as input a date, HomeTeam and AwayTeam and output a filtered dataframe where the matches shown are played before input data and match is between HomeTeam and AwayTeam\n",
    "\n",
    "# For Example:\n",
    "# date = \"24/06/2020\"\n",
    "# HomeTeam = \"Newcastle\"\n",
    "# AwayTeam = \"Aston Villa\"\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if date.month <= 7:\n",
    "        return datetime(date.year-3, 8, 1)\n",
    "    return datetime(date.year-2, 8, 1)\n",
    "\n",
    "def filter_dataframe_by_bothteams_history(df, date, HomeTeam, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date<input(Date) && HomeTeam=input(HomeTeam) && AwayTeam=input(AwayTeam)\n",
    "    df_filtered = df_epl_train.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.HomeTeam==HomeTeam) & (df_filtered.AwayTeam==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_hometeam_recent_season(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date<input(Date) && Date>input(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.HomeTeam==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_recent_season(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date<input(Date) && Date>input(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.AwayTeam==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "# An example to see what the function does:\n",
    "print(filter_dataframe_by_hometeam_recent_season(df_epl_train, \"24/06/2020\", \"Newcastle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the average of each of the columns that we need from this filtered dataframe e.g. HST, AST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input the filtered dataframe from previous cell, features to average and a dictionary,\n",
    "# it then appends an average of each feature to the dictionary\n",
    "\n",
    "def average_columns(avg_features, filtered_df):\n",
    "    for feature in avg_features.keys():\n",
    "        df_col_means = filtered_df[feature].mean()\n",
    "        avg_features[feature].append(df_col_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the two functions on each row of the original dataframe to fill the dictionary with averages for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the two functions for each row of our df_epl_train dataframe to fill dictionary with AVG for each match\n",
    "# NOTE: Some matches won't have past stats since the two teams may not have played against each other in past or we might not have the data\n",
    "\n",
    "# These are the features we want to get averages for both teams\n",
    "# avg_features_both = {\n",
    "#                     \"FTHG\": [],\n",
    "#                     \"FTAG\": [],\n",
    "#                     \"HTHG\": [],\n",
    "#                     \"HTAG\": [],\n",
    "#                     \"HS\"  : [],\n",
    "#                     \"AS\"  : [],\n",
    "#                     \"HST\" : [],\n",
    "#                     \"AST\" : [],\n",
    "#                     \"HF\"  : [],\n",
    "#                     \"AF\"  : [],\n",
    "#                     \"HC\"  : [],\n",
    "#                     \"AC\"  : [],\n",
    "#                     \"HY\"  : [],\n",
    "#                     \"AY\"  : [],\n",
    "#                     \"HR\"  : [],\n",
    "#                     \"AR\"  : []\n",
    "#                 }\n",
    "avg_features_both = {\n",
    "                    \"HS\"  : [],\n",
    "                    \"AS\"  : [],\n",
    "                    \"HST\" : [],\n",
    "                    \"AST\" : [],\n",
    "                    \"HF\"  : [],\n",
    "                    \"AF\"  : [],\n",
    "                    \"HC\"  : [],\n",
    "                    \"AC\"  : [],\n",
    "                    \"HY\"  : [],\n",
    "                    \"AY\"  : [],\n",
    "                    \"HR\"  : [],\n",
    "                    \"AR\"  : []\n",
    "                }\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "# avg_features_home = {\n",
    "#                     \"FTHG\": [],\n",
    "#                     \"HTHG\": [],\n",
    "#                     \"HS\"  : [],\n",
    "#                     \"HST\" : [],\n",
    "#                     \"HF\"  : [],\n",
    "#                     \"HC\"  : [],\n",
    "#                     \"HY\"  : [],\n",
    "#                     \"HR\"  : [],\n",
    "#                 }\n",
    "avg_features_home = {\n",
    "                    \"HS\"  : [],\n",
    "                    \"HST\" : [],\n",
    "                    \"HF\"  : [],\n",
    "                    \"HC\"  : [],\n",
    "                    \"HY\"  : [],\n",
    "                    \"HR\"  : [],\n",
    "                }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "# avg_features_away = {\n",
    "#                     \"FTAG\": [],\n",
    "#                     \"HTAG\": [],\n",
    "#                     \"AS\"  : [],\n",
    "#                     \"AST\" : [],\n",
    "#                     \"AF\"  : [],\n",
    "#                     \"AC\"  : [],\n",
    "#                     \"AY\"  : [],\n",
    "#                     \"AR\"  : []\n",
    "#                   }\n",
    "avg_features_away = {\n",
    "                    \"AS\"  : [],\n",
    "                    \"AST\" : [],\n",
    "                    \"AF\"  : [],\n",
    "                    \"AC\"  : [],\n",
    "                    \"AY\"  : [],\n",
    "                    \"AR\"  : []\n",
    "                }\n",
    "\n",
    "\n",
    "# Run the two functions on each row of the df_epl_train and fill the dictionary\n",
    "# For each row in the dataframe\n",
    "for index, row in df_epl_train.iterrows():\n",
    "    # Filter the dataframe to only show matches played between those teams and before the certain date\n",
    "    df_epl_train_average_bothteams_history = filter_dataframe_by_bothteams_history(df_epl_train, row[\"Date\"],row[\"HomeTeam\"],row[\"AwayTeam\"])\n",
    "    df_epl_train_average_hometeam_recent_season = filter_dataframe_by_hometeam_recent_season(df_epl_train, row[\"Date\"],row[\"HomeTeam\"])\n",
    "    df_epl_train_average_awayteam_recent_season = filter_dataframe_by_awayteam_recent_season(df_epl_train, row[\"Date\"],row[\"AwayTeam\"])\n",
    "    # Get averages from the filtered dataframe and add the the dictionary\n",
    "    average_columns(avg_features_both, df_epl_train_average_bothteams_history)\n",
    "    average_columns(avg_features_home, df_epl_train_average_hometeam_recent_season)\n",
    "    average_columns(avg_features_away, df_epl_train_average_awayteam_recent_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_features_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add these average feature lists in the dictionary back into the original dataframe as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for each of these feature averages using our list of values from the dictionary\n",
    "df_epl_train_updated = df_epl_train.copy()\n",
    "\n",
    "for feature in avg_features_both.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_both[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl_train_updated[feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "for feature in avg_features_home.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl_train_updated[feature + \"_AVG\"] = feature_vals\n",
    "    \n",
    "for feature in avg_features_away.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl_train_updated[feature + \"_AVG\"] = feature_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the new dataframe with the added columns with past average statistics for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this dataframe contains our original data + the average of the past stats for each row\n",
    "print(df_epl_train_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Some rows have nan values, these are matches where teams might not have played each other before (or we dont have the past match data for them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to remove these rows (from the dataframe) where there are nan values. This is required to run the classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We must remove the rows in the dataframe where the average values of stats/features are 'nan';\n",
    "# we get these values because either the two teams have not played a match in the past OR because we have\n",
    "# not got the past stats for these matches. We cannot use the 'nan' values for the classifier training and \n",
    "# hence have to remove these rows. We can then train a classifier using this final dataframe.\n",
    "\n",
    "# In the final model/classifier, in the case where we DO NOT have these past stats of the two teams playing, \n",
    "# we need to switch back to using the OLD classifier which only took the 4 basic fetaures: day, month, \n",
    "# HomeTeam and AwayTeam.\n",
    "\n",
    "# In the case where we DO have these past stats for two teams playing each other, we can use this model/classifier \n",
    "# and input the features like HST_AVG and AST_AVG. We would find these by using the filter_dataframe() and \n",
    "# average_columns() functions to find them for any two specific teams playing each other on some date.\n",
    "\n",
    "# Remove any rows with nan\n",
    "df_epl_train_final = df_epl_train_updated.dropna()\n",
    "df_epl_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn the catergorical data into labels using same method from before\n",
    "df_epl_train_final[\"AwayTeam_Enc\"] = df_epl_train_final[\"AwayTeam\"].astype(\"category\").cat.codes\n",
    "df_epl_train_final[\"HomeTeam_Enc\"] = df_epl_train_final[\"HomeTeam\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "df_epl_train_final[\"Date\"] = pd.to_datetime(df_epl_train_final[\"Date\"])\n",
    "df_epl_train_final[\"Day\"] = df_epl_train_final[\"Date\"].dt.day\n",
    "df_epl_train_final[\"Month\"] = df_epl_train_final[\"Date\"].dt.month \n",
    "df_epl_train_final[\"Year\"] = df_epl_train_final[\"Date\"].dt.year\n",
    "\n",
    "# Check the final updated dataframe\n",
    "df_epl_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our X and y matrix and split into a training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input features matrix X (made of day, month, HomeTeam, AwayTeam, FTHG_AVG, FTAG_AVG, etc)\n",
    "# Create the output values y vector (made of FTR)\n",
    "# Take these values from the transformed dataframe\n",
    "\n",
    "# X = df_epl_train_final.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc','FTHG_AVG','FTAG_AVG','HTHG_AVG','HTAG_AVG','HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG','HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG']].values\n",
    "X = df_epl_train_final.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc',\n",
    "                              'HS_HISTORY','AS_HISTORY','HST_HISTORY','AST_HISTORY','HF_HISTORY','AF_HISTORY','HC_HISTORY','AC_HISTORY','HY_HISTORY','AY_HISTORY','HR_HISTORY','AR_HISTORY',\n",
    "                              'HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG','HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG']].values\n",
    "y = df_epl_train_final.loc[:,'FTR'].values\n",
    "\n",
    "# Split the training data in a 80-20 split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "# Encode the y output values as well\n",
    "FTR_encoder = LabelEncoder()\n",
    "y_train = FTR_encoder.fit_transform(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test using the different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Tree model\n",
    "DT_Model = DecisionTreeClassifier(random_state=42)\n",
    "# Fit the model using training data\n",
    "DT_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "DT_predictions_test = DT_Model.predict(X_test)\n",
    "# Reconverting prediction values (i.e. 0, 1 or 2) back into (H, D or A) using the FTR_encoder defined in earlier cell\n",
    "DT_predictions_test = FTR_encoder.inverse_transform(DT_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(DT_predictions_test, y_test))\n",
    "print(classification_report(DT_predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty KNN model\n",
    "KNN_Model = KNeighborsClassifier(n_neighbors=6)\n",
    "# Fit the model using training data\n",
    "KNN_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "KNN_predictions_test = KNN_Model.predict(X_test)\n",
    "KNN_predictions_test = FTR_encoder.inverse_transform(KNN_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(KNN_predictions_test, y_test))\n",
    "print(classification_report(KNN_predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Random Forest model\n",
    "RF_Model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "# Fit the model using training data\n",
    "RF_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "RF_predictions_test = RF_Model.predict(X_test)\n",
    "RF_predictions_test = FTR_encoder.inverse_transform(RF_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(RF_predictions_test, y_test))\n",
    "print(classification_report(RF_predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8405811c74f0ffe7939ee5cd32d38284a352f8814caa292a26dc0eead285378f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
