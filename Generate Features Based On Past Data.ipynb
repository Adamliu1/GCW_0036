{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6e135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0579316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.path.join(os.getcwd(), 'Group Coursework Brief-20221106', 'Data_Files', 'Data_Files')\n",
    "dirName_trainData = os.path.join(cwd, 'epl-training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19785c8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/neilbadal/Desktop/COMP0036 CW/Group Coursework Brief-20221106/Data_Files/Data_Files/epl-training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_epl_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dirName_trainData)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/neilbadal/Desktop/COMP0036 CW/Group Coursework Brief-20221106/Data_Files/Data_Files/epl-training.csv'"
     ]
    }
   ],
   "source": [
    "df_epl_train = pd.read_csv(dirName_trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bca69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ec63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column from strings into datetime objects\n",
    "df_epl_train[\"Date\"] = pd.to_datetime(df_epl_train[\"Date\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c286979",
   "metadata": {},
   "source": [
    "AIM: A function that takes as input the date, HomeTeam and AwayTeam. It will filter the df_epl_train dataframe for matches between HomeTeam and AwayTeam that took place before the input date. Then take an average of the \n",
    "columns like HR, AR, etc. This will provide us with the past stats for games played in past between the two teams. We can then use these past stats (between the two teams) as features to input into the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b6612",
   "metadata": {},
   "source": [
    "First, filter the dataframe to include only matches where date is less than date specified and also only include matches where HomeTeam=input(HomeTeam) and AwayTeam=input(AwayTeam):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6566f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take as input a date, HomeTeam and AwayTeam and output a filtered dataframe where the matches shown are played before input data and match is between HomeTeam and AwayTeam\n",
    "\n",
    "# For Example:\n",
    "# date = \"24/06/2020\"\n",
    "# HomeTeam = \"Newcastle\"\n",
    "# AwayTeam = \"Aston Villa\"\n",
    "\n",
    "def filter_dataframe(date, HomeTeam, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date<input(Date) && HomeTeam=input(HomeTeam) && AwayTeam=input(AwayTeam)\n",
    "    df_epl_train_filtered = df_epl_train.copy()\n",
    "    df_epl_train_filtered = df_epl_train_filtered[(df_epl_train.Date<date) & (df_epl_train.HomeTeam==HomeTeam) & (df_epl_train.AwayTeam==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_epl_train_filtered\n",
    "\n",
    "# An example to see what the function does:\n",
    "print(filter_dataframe(\"24/06/2020\", \"Newcastle\", \"Aston Villa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583df9a",
   "metadata": {},
   "source": [
    "We now find the average of each of the columns that we need from this filtered dataframe e.g. HST, AST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11766251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input the filtered dataframe from previous cell, features to average and a dictionary,\n",
    "# it then appends an average of each feature to the dictionary\n",
    "\n",
    "def average_columns(features, avg_features, filtered_df):\n",
    "    for feature in features:\n",
    "        df_col_means = df_epl_train_filtered[feature].mean()\n",
    "        avg_features[feature].append(df_col_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ffbb3",
   "metadata": {},
   "source": [
    "We now run the two functions on each row of the original dataframe to fill the dictionary with averages for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the two functions for each row of our df_epl_train dataframe to fill dictionary with AVG for each match\n",
    "# NOTE: Some matches won't have past stats since the two teams may not have played against each other in past or we might not have the data\n",
    "\n",
    "# These are the features we want to get averages for\n",
    "features = [\"FTHG\",\"FTAG\",\"HTHG\",\"HTAG\",\"HS\",\"AS\",\"HST\",\"AST\",\"HF\",\"AF\",\"HC\",\"AC\",\"HY\",\"AY\",\"HR\",\"AR\"]\n",
    "avg_features = {\n",
    "                    \"FTHG\": [],\n",
    "                    \"FTAG\": [],\n",
    "                    \"HTHG\": [],\n",
    "                    \"HTAG\": [],\n",
    "                    \"HS\"  : [],\n",
    "                    \"AS\"  : [],\n",
    "                    \"HST\" : [],\n",
    "                    \"AST\" : [],\n",
    "                    \"HF\"  : [],\n",
    "                    \"AF\"  : [],\n",
    "                    \"HC\"  : [],\n",
    "                    \"AC\"  : [],\n",
    "                    \"HY\"  : [],\n",
    "                    \"AY\"  : [],\n",
    "                    \"HR\"  : [],\n",
    "                    \"AR\"  : []\n",
    "                }\n",
    "\n",
    "# Run the two functions on each row of the df_epl_train and fill the dictionary\n",
    "# For each row in the dataframe\n",
    "for index, row in df_epl_train.iterrows():\n",
    "    # Filter the dataframe to only show matches played between those teams and before the certain date\n",
    "    df_epl_train_filtered = filter_dataframe(row[\"Date\"],row[\"HomeTeam\"],row[\"AwayTeam\"])\n",
    "    # Get averages from the filtered dataframe and add the the dictionary\n",
    "    average_columns(features, avg_features, df_epl_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bfdc1",
   "metadata": {},
   "source": [
    "Add these average feature lists in the dictionary back into the original dataframe as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for each of these feature averages using our list of values from the dictionary\n",
    "df_epl_train_updated = df_epl_train.copy()\n",
    "features = [\"FTHG\",\"FTAG\",\"HTHG\",\"HTAG\",\"HS\",\"AS\",\"HST\",\"AST\",\"HF\",\"AF\",\"HC\",\"AC\",\"HY\",\"AY\",\"HR\",\"AR\"]\n",
    "\n",
    "for feature in features:\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl_train_updated[feature + \"_AVG\"] = feature_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74bd7a",
   "metadata": {},
   "source": [
    "This is the new dataframe with the added columns with past average statistics for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this dataframe contains our original data + the average of the past stats for each row\n",
    "df_epl_train_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ad295",
   "metadata": {},
   "source": [
    "NOTE: Some rows have nan values, these are matches where teams might not have played each other before (or we dont have the past match data for them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15726c2c",
   "metadata": {},
   "source": [
    "So we need to remove these rows (from the dataframe) where there are nan values. This is required to run the classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965264ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We must remove the rows in the dataframe where the average values of stats/features are 'nan';\n",
    "# we get these values because either the two teams have not played a match in the past OR because we have\n",
    "# not got the past stats for these matches. We cannot use the 'nan' values for the classifier training and \n",
    "# hence have to remove these rows. We can then train a classifier using this final dataframe.\n",
    "\n",
    "# In the final model/classifier, in the case where we DO NOT have these past stats of the two teams playing, \n",
    "# we need to switch back to using the OLD classifier which only took the 4 basic fetaures: day, month, \n",
    "# HomeTeam and AwayTeam.\n",
    "\n",
    "# In the case where we DO have these past stats for two teams playing each other, we can use this model/classifier \n",
    "# and input the features like HST_AVG and AST_AVG. We would find these by using the filter_dataframe() and \n",
    "# average_columns() functions to find them for any two specific teams playing each other on some date.\n",
    "\n",
    "# Remove any rows with nan\n",
    "df_epl_train_final = df_epl_train_updated.dropna()\n",
    "df_epl_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0a891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn the catergorical data into labels using same method from before\n",
    "df_epl_train_final[\"AwayTeam_Enc\"] = df_epl_train_final[\"AwayTeam\"].astype(\"category\").cat.codes\n",
    "df_epl_train_final[\"HomeTeam_Enc\"] = df_epl_train_final[\"HomeTeam\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "df_epl_train_final[\"Date\"] = pd.to_datetime(df_epl_train_final[\"Date\"])\n",
    "df_epl_train_final[\"Day\"] = df_epl_train_final[\"Date\"].dt.day\n",
    "df_epl_train_final[\"Month\"] = df_epl_train_final[\"Date\"].dt.month \n",
    "df_epl_train_final[\"Year\"] = df_epl_train_final[\"Date\"].dt.year\n",
    "\n",
    "# Check the final updated dataframe\n",
    "df_epl_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d3166",
   "metadata": {},
   "source": [
    "Create our X and y matrix and split into a training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba334f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input features matrix X (made of day, month, HomeTeam, AwayTeam, FTHG_AVG, FTAG_AVG, etc)\n",
    "# Create the output values y vector (made of FTR)\n",
    "# Take these values from the transformed dataframe\n",
    "\n",
    "X = df_epl_train_final.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc','FTHG_AVG','FTAG_AVG','HTHG_AVG','HTAG_AVG','HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG','HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG']].values\n",
    "y = df_epl_train_final.loc[:,'FTR'].values\n",
    "\n",
    "# Split the training data in a 80-20 split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "# Encode the y output values as well\n",
    "FTR_encoder = LabelEncoder()\n",
    "y_train = FTR_encoder.fit_transform(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7731af",
   "metadata": {},
   "source": [
    "Now we can test using the different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Tree model\n",
    "DT_Model = DecisionTreeClassifier(random_state=42)\n",
    "# Fit the model using training data\n",
    "DT_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "DT_predictions_test = DT_Model.predict(X_test)\n",
    "# Reconverting prediction values (i.e. 0, 1 or 2) back into (H, D or A) using the FTR_encoder defined in earlier cell\n",
    "DT_predictions_test = FTR_encoder.inverse_transform(DT_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(DT_predictions_test, y_test))\n",
    "print(classification_report(DT_predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty KNN model\n",
    "KNN_Model = KNeighborsClassifier(n_neighbors=6)\n",
    "# Fit the model using training data\n",
    "KNN_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "KNN_predictions_test = KNN_Model.predict(X_test)\n",
    "KNN_predictions_test = FTR_encoder.inverse_transform(KNN_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7faf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(KNN_predictions_test, y_test))\n",
    "print(classification_report(KNN_predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Random Forest model\n",
    "RF_Model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "# Fit the model using training data\n",
    "RF_Model.fit(X_train, y_train)\n",
    "# Make predictions using the model we have created\n",
    "RF_predictions_test = RF_Model.predict(X_test)\n",
    "RF_predictions_test = FTR_encoder.inverse_transform(RF_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(RF_predictions_test, y_test))\n",
    "print(classification_report(RF_predictions_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8405811c74f0ffe7939ee5cd32d38284a352f8814caa292a26dc0eead285378f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
