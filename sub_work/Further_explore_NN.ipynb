{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[COMP0036 Group Assignment: BEAT THE BOOKIE](#toc0_)\n",
    "\n",
    "## <a id='toc1_1_'></a>[Group N - Introduction](#toc0_)\n",
    "\n",
    "We have been assigned to build model(s) that predict the FTR value, which can be Home Win (H), Draw (D) and Away Win (A). The general steps we will be taking to build the model(s) begins with finding a suitable dataset and performing feature engineering on the selected features to be used in the model. This entails creating functions or classes to convert the raw data and transforms it into a format where every match has that historic feature. Then, we perform feature selection to filter out unimportant features, and use the selected features in model(s), and then compare and decide the best performing model. Finally, improve models to get the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [COMP0036 Group Assignment: BEAT THE BOOKIE](#toc1_)    \n",
    "  - [Group N - Introduction](#toc1_1_)    \n",
    "  - [Index](#toc1_2_)    \n",
    "- [Data Import](#toc2_)    \n",
    "  - [Libraries & data source define](#toc2_1_)    \n",
    "    - [Libraries](#toc2_1_1_)    \n",
    "    - [RUNNING Config (GPU/CPU)](#toc2_1_2_)    \n",
    "    - [FLAGS TO DISABLE false positive warnings](#toc2_1_3_)    \n",
    "    - [Data Source path](#toc2_1_4_)    \n",
    "  - [Raw data loading and inspection](#toc2_2_)    \n",
    "- [Data Transformation & Exploration](#toc3_)    \n",
    "  - [Initial transformations](#toc3_1_)    \n",
    "    - [Replacing 'Date' strings with DateTime objects](#toc3_1_1_)    \n",
    "    - [Adding standings and rankings data to the Dataframe](#toc3_1_2_)    \n",
    "    - [Adding manager data to the Dataframe](#toc3_1_3_)    \n",
    "    - [Encoding Categorical Data](#toc3_1_4_)    \n",
    "  - [Data Exploration](#toc3_2_)    \n",
    "- [Feature Engineering](#toc4_)    \n",
    "    - [Adding Average Past Match Statistics & Past Season % Number Of Wins](#toc4_1_1_)    \n",
    "    - [Adding Expected Goals](#toc4_1_2_)    \n",
    "    - [Removing Pre-encoded Data](#toc4_1_3_)    \n",
    "  - [Breakdown of Features In The Dataframe/Dataset](#toc4_2_)    \n",
    "  - [Final Dataframe containing all features](#toc4_3_)    \n",
    "    - [PLOT COOR MATRIX AGAIN (DEBUG)](#toc4_3_1_)    \n",
    "- [Auxiliary Functions + Classifier interfaces](#toc5_)    \n",
    "  - [Evaluation helpers](#toc5_1_)    \n",
    "  - [Plotting helpers](#toc5_2_)    \n",
    "  - [Metrics and classifiers](#toc5_3_)    \n",
    "    - [scoring metrics and define cross validation data split](#toc5_3_1_)    \n",
    "      - [helper function for producing report](#toc5_3_1_1_)    \n",
    "    - [Classifiers](#toc5_3_2_)    \n",
    "      - [Random Guesses](#toc5_3_2_1_)    \n",
    "      - [Decision Tree Classifier](#toc5_3_2_2_)    \n",
    "      - [Random Forest Classifier](#toc5_3_2_3_)    \n",
    "      - [K-Nearest Neighbours (KNN) Classifier](#toc5_3_2_4_)    \n",
    "      - [Support Vector Machine (SVM) Classifier](#toc5_3_2_5_)    \n",
    "      - [XGB](#toc5_3_2_6_)    \n",
    "      - [Neural Network](#toc5_3_2_7_)    \n",
    "        - [Build function of the NN](#toc5_3_2_7_1_)    \n",
    "      - [Feature set interfaces](#toc5_3_2_8_)    \n",
    "        - [without cross-validation](#toc5_3_2_8_1_)    \n",
    "        - [with cross-validation](#toc5_3_2_8_2_)    \n",
    "- [Model Selection via Cross Validation](#toc6_)    \n",
    "    - [Introduction](#toc6_1_1_)    \n",
    "    - [FEATURE SET 5 (MANUAL) – WITH Model Selection](#toc6_1_2_)    \n",
    "      - [Create Design Matrix](#toc6_1_2_1_)    \n",
    "  - [CONTINUE ON BEST FEATURE SET & EXPLORE MORE ON NN](#toc6_2_)    \n",
    "      - [redefine summary info](#toc6_2_1_1_)    \n",
    "    - [Aux Functions](#toc6_2_2_)    \n",
    "      - [Build function](#toc6_2_2_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Index](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Data Import](#toc0_)\n",
    "Here we import the libraries and define the data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Libraries & data source define](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import operator\n",
    "import random\n",
    "from calendar import month_name\n",
    "# import seaborn as sns\n",
    "from pandas.core.common import random_state\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[RUNNING Config (GPU/CPU)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out this cell if not using GPU acceleration\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpu_devices)\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# This statement is used to log if tf is using GPU\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA\n",
    "!conda list cudatoolkit\n",
    "\n",
    "!conda list cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[FLAGS TO DISABLE false positive warnings](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[Data Source path](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName_matchData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/epl-full-training.csv'\n",
    "dirName_rankingData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/EPL%20Standings%202000-2022.csv'\n",
    "dirName_managerData = 'https://raw.githubusercontent.com/shabir-dhillon/GCW_0036/main/Group%20Coursework%20Brief-20221106/Data_Files/epl-manager-data-V3.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Raw data loading and inspection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the main csv file\n",
    "df_epl = pd.read_csv(dirName_matchData)\n",
    "# Check the raw data\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the standings csv file\n",
    "df_ranking = pd.read_csv(dirName_rankingData)\n",
    "df_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Write short description about what this dataset represents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the manager csv file\n",
    "df_manager = pd.read_csv(dirName_managerData, encoding=\"ISO-8859-1\")\n",
    "df_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Transformation & Exploration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Initial transformations](#toc0_)\n",
    "\n",
    "### <a id='toc3_1_1_'></a>[Replacing 'Date' strings with DateTime objects](#toc0_)\n",
    "\n",
    "In the raw dataset, the `'Date'` column has a `string` type. Converting it to DateTime object will allow easier usage. It also allows for extracting our first features - day, month and year of the game. We will analyse their importance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"], dayfirst=True)\n",
    "\n",
    "df_epl[\"Day\"] = df_epl[\"Date\"].dt.day\n",
    "df_epl[\"Month\"] = df_epl[\"Date\"].dt.month \n",
    "df_epl[\"Year\"] = df_epl[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Adding standings and rankings data to the Dataframe](#toc0_)\n",
    "\n",
    "The following snippet is used to combine match detailed dataframe - `df_epl`, with team seasonal standings and rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - HELPER FUNCTIONS:\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if int(date.month) <= 7:\n",
    "      return datetime(int(date.year)-1, 8, 1)\n",
    "    return datetime(int(date.year), 8, 1)\n",
    "\n",
    "\n",
    "# PART 2 - ADDING STANDINGS DATA TO df_epl:\n",
    "\n",
    "homeTeamRankings = []\n",
    "awayTeamRankings = []\n",
    "diffRankings = []\n",
    "\n",
    "for index, row in df_epl.iterrows():\n",
    "    season = get_season_start_date(row[\"Date\"])\n",
    "    prev_season = str(season.year-1) + \"-\" + str(season.year)[-2:] # from df_ranking\n",
    "    homeTeam = row[\"HomeTeam\"]    \n",
    "    awayTeam = row[\"AwayTeam\"]\n",
    "\n",
    "    #filter dataframe ranking for year=season and hometeam=homeTeam\n",
    "    df_epl_train_filtered_H = df_ranking.copy()\n",
    "    df_epl_train_filtered_H = df_epl_train_filtered_H[(df_ranking.Season==prev_season) & (df_ranking.Team==homeTeam)]\n",
    "\n",
    "    #filter dataframe ranking for year=season and awayTeam=awayTeam\n",
    "    df_epl_train_filtered_A = df_ranking.copy()\n",
    "    df_epl_train_filtered_A = df_epl_train_filtered_A[(df_ranking.Season==prev_season) & (df_ranking.Team==awayTeam)]\n",
    "    \n",
    "    # To keep df consistent, we add Nan to row, so we could remove it later by using df.dropna()\n",
    "    # TODO: refactory\n",
    "    # 18 represents team that been promoted from previous league this year. Hence, they don't have ranking from previous league\n",
    "    # Pos is embbed inside pd.Series object, use values to access values of series.\n",
    "    # Because Pos should be a int, hence, values always has length 1 or 0 (no data)\n",
    "    if df_epl_train_filtered_H.empty and df_epl_train_filtered_A.empty:\n",
    "        homeRanking = 18\n",
    "        awayRanking = 18\n",
    "    elif df_epl_train_filtered_H.empty and (not df_epl_train_filtered_A.empty):\n",
    "        homeRanking = 18\n",
    "        awayRanking = df_epl_train_filtered_A['Pos'].values[0]\n",
    "    elif (not df_epl_train_filtered_H.empty) and df_epl_train_filtered_A.empty:\n",
    "        homeRanking = df_epl_train_filtered_H['Pos'].values[0]\n",
    "        awayRanking = 18\n",
    "    else:\n",
    "        homeRanking = df_epl_train_filtered_H['Pos'].values[0]\n",
    "        awayRanking = df_epl_train_filtered_A['Pos'].values[0]\n",
    "\n",
    "    homeTeamRankings.append(homeRanking)\n",
    "    awayTeamRankings.append(awayRanking)\n",
    "    diffRankings.append(homeRanking-awayRanking)\n",
    "\n",
    "df_epl['HomeStanding_PrevSeason'] = homeTeamRankings\n",
    "df_epl['AwayStanding_PrevSeason'] = awayTeamRankings\n",
    "df_epl['DiffStanding_PrevSeason'] = diffRankings\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Adding manager data to the Dataframe](#toc0_)\n",
    "\n",
    "The following snippet is used to combine the existing dataframe - `df_epl`, with the third dataset consisting team managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - CLEAN MANAGER DATA:\n",
    "\n",
    "# Check the original manager data - note nan's in 'End Date' indicate that manager is still at club\n",
    "# Here we update the nan values in the dataframe to the current date today\n",
    "updatedEndDates = []\n",
    "now = datetime.now() # Get current time\n",
    "today = now.strftime(\"%d/%m/%Y\")\n",
    "for index, row in df_manager.iterrows():\n",
    "    date =  row['End Date']\n",
    "    if pd.isnull(date):\n",
    "        updatedEndDates.append(today)\n",
    "    else:\n",
    "        updatedEndDates.append(date)\n",
    "        \n",
    "# Add these updated End_Dates to the dataframe\n",
    "df_manager['End_Date'] = updatedEndDates\n",
    "\n",
    "# Change the names of column index to include '_' instead of space and convert string dates into datetime objects\n",
    "df_manager['Start_Date'] = pd.to_datetime(df_manager[\"Start Date\"], dayfirst=True)\n",
    "df_manager['End_Date'] = pd.to_datetime(df_manager[\"End_Date\"], dayfirst=True)\n",
    "\n",
    "\n",
    "\n",
    "# PART 2 - ADDING MANAGER DATA TO df_epl:\n",
    "\n",
    "homeTeamManagers = []\n",
    "awayTeamManagers = []\n",
    "\n",
    "# Here for each row in the df_epl we will add the manager for home and away team\n",
    "for index, row in df_epl.iterrows():\n",
    "    date =  row['Date']\n",
    "    homeTeam = row[\"HomeTeam\"]\n",
    "    awayTeam = row[\"AwayTeam\"]\n",
    "\n",
    "    #filter dataframe ranking for date between start and end of managers and hometeam=homeTeam\n",
    "    df_epl_train_filtered_H = df_manager.copy()\n",
    "    df_epl_train_filtered_H = df_epl_train_filtered_H[(df_manager.Start_Date<=date) & (df_manager.End_Date>=date) & (df_manager.Club==homeTeam)]\n",
    "\n",
    "    #filter dataframe ranking for date between start and end of managers and awayTeam=awayTeam\n",
    "    df_epl_train_filtered_A = df_manager.copy()\n",
    "    df_epl_train_filtered_A = df_epl_train_filtered_A[(df_manager.Start_Date<=date) & (df_manager.End_Date>=date) & (df_manager.Club==awayTeam)]\n",
    "    \n",
    "    # NOTE - we get some managers as anonymous - this is happening when there is a change of manager (old manager has left, new manager has not started yet) and a match takes place\n",
    "    if df_epl_train_filtered_H.empty:\n",
    "        homeManager = \"Anonymous\"\n",
    "    else:\n",
    "        homeManager = df_epl_train_filtered_H['Name'].values[0]\n",
    "        \n",
    "    if df_epl_train_filtered_A.empty:\n",
    "        awayManager = \"Anonymous\"\n",
    "    else:\n",
    "        awayManager = df_epl_train_filtered_A['Name'].values[0]\n",
    "    \n",
    "    homeTeamManagers.append(homeManager)\n",
    "    awayTeamManagers.append(awayManager)\n",
    "\n",
    "df_epl['Home_Manager'] = homeTeamManagers\n",
    "df_epl['Away_Manager'] = awayTeamManagers\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_4_'></a>[Encoding Categorical Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models accept only numbers as their input, so we need to encode all categorical data like team names, referees and managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if home teams are the same as away teams, same for managers\n",
    "print(set(df_epl['AwayTeam'].unique()) == set(df_epl['HomeTeam'].unique()))\n",
    "print(set(df_epl['Home_Manager'].unique()) == set(df_epl['Away_Manager'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categorical string data to integer values\n",
    "team_encoder = LabelEncoder()\n",
    "df_epl[\"HomeTeam_Enc\"] = team_encoder.fit_transform(df_epl['HomeTeam'])\n",
    "df_epl[\"AwayTeam_Enc\"] = team_encoder.transform(df_epl['AwayTeam'])\n",
    "\n",
    "referee_encoder = LabelEncoder()\n",
    "df_epl[\"Referee_Enc\"] = referee_encoder.fit_transform(df_epl['Referee'])\n",
    "\n",
    "FTR_encoder = LabelEncoder()\n",
    "df_epl[\"FTR_Enc\"] = FTR_encoder.fit_transform(df_epl[\"FTR\"])\n",
    "df_epl[\"HTR_Enc\"] = FTR_encoder.transform(df_epl[\"HTR\"])\n",
    "\n",
    "manager_encoder = LabelEncoder()\n",
    "manager_encoder.fit(list(set(df_epl['Home_Manager'].unique()).union(set(df_epl['Away_Manager'].unique()))))\n",
    "df_epl[\"Home_Manager_Enc\"] = manager_encoder.transform(df_epl['Home_Manager'])\n",
    "df_epl[\"Away_Manager_Enc\"] = manager_encoder.transform(df_epl['Away_Manager'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Data Exploration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting and engineering features, we explore the dataset to learn about its specifics. Firstly, we check for visible outliers, existing NaN values and have a look at the output of the `.describe()` summary for all numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are no NaN values in the dataset. Quick glance at other statistcs gives hope that the data is not corrupted. Let's check if this classification problem can be considered balanced by plotting a histogram of classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_epl, x='FTR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df = df_epl.describe()\n",
    "# Graph inspired by https://armantee.github.io/predicting/\n",
    "\n",
    "home_features = ['FTHG','HTHG','HS','HST','HC','HF','HY','HR']\n",
    "away_features = ['FTAG','HTAG','AS','AST','AC','AF','AY','AR']\n",
    "fig=plt.figure(figsize=(18, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# width of the bars\n",
    "barWidth = 0.2\n",
    "bars1 = np.array(describe_df.iloc[1, :][home_features]).flatten()\n",
    "bars2 = np.array(describe_df.iloc[1, :][away_features]).flatten()\n",
    "yer1 = np.array(describe_df.iloc[2, :][home_features]).flatten()\n",
    "yer2 = np.array(describe_df.iloc[2, :][away_features]).flatten()\n",
    "# The x position of bars\n",
    "r1 = np.arange(len(bars1.flatten()))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "# Create blue bars\n",
    "plt.bar(r1, bars1, width = barWidth, color = 'orange', edgecolor = 'black', yerr=yer1, capsize=7, label='Home team')\n",
    "# Create cyan bars\n",
    "plt.bar(r2, bars2, width = barWidth, color = 'green', edgecolor = 'black', yerr=yer2, capsize=7, label='Away team')\n",
    "# general layout\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['Goals Scored', 'Half-time Goals Scored', 'Shots','Shots on target', 'Corners','Fouls','Yellow Cards','Red Cards'])\n",
    "plt.ylabel('Average Value')\n",
    "plt.ylim(0)\n",
    "plt.title(\"Average values for features split between home and away team.\")\n",
    "plt.legend()\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most of standard deviation errors overlap greatly, the home advantage is clearly visible. This needs to be taken into account while working on classifiers. Shots and Shots on target may be especially great indicators for the result. Let's verify this and explore the correlation matrix for the existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(df_epl.corr(numeric_only=\"False\"), cmap=\"PiYG\", annot= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_dict = []\n",
    "for i in range(2002, 2023):\n",
    "    grouped_wins = df_epl[(df_epl['FTR'] == 'H') & (df_epl['Year'] == i)].groupby('HomeTeam')\n",
    "    grouped_all = df_epl[(df_epl['Year'] == i)].groupby('HomeTeam')\n",
    "    \n",
    "    year_dict.append(dict(grouped_wins['FTR'].count()/grouped_all['FTR'].count()))\n",
    "\n",
    "def get_team_stats(team_name):\n",
    "    team = []\n",
    "    for year in range(len(year_dict)):\n",
    "        try:\n",
    "            value = year_dict[year][team_name]\n",
    "            if np.isnan(value):\n",
    "                value = 0\n",
    "        except KeyError:\n",
    "            value = 0\n",
    "        team.append(value)\n",
    "    return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(14, 3, figsize=(12, 40), constrained_layout=True)\n",
    "\n",
    "for i, team_name in enumerate(df_epl['HomeTeam'].unique()):\n",
    "    ax[i//3][i%3].set_title(team_name)\n",
    "    ax[i//3][i%3].grid()\n",
    "    # ax[i//3][i%3].set_xticks(np.arange(2002, 2023, step=1))\n",
    "    ax[i//3][i%3].set_ylim(ymin=0)\n",
    "    y = get_team_stats(team_name)\n",
    "    x = np.array((range(2002, 2023)))\n",
    "    ax[i//3][i%3].plot(x, y, '-o')\n",
    "    if 0 not in set(y):\n",
    "        a, b = np.polyfit(x, y, 1)\n",
    "        ax[i//3][i%3].plot(x, a*x+b)\n",
    "fig.suptitle(\"Team performance over years - percentage of won matches\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that most of the teams did not play for the full 20 past years. This is likely caused by league promotions and relegations. \n",
    "\n",
    "Let's see if a time of a year has an influence on a match outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "month_names = {}\n",
    "for month in range(1, 13):\n",
    "    h = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'H')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    d = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'D')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    a = 100 * len(df_epl[(df_epl['Month'] == month) & (df_epl['FTR'] == 'A')])/len(df_epl[(df_epl['Month'] == month)])\n",
    "    counts[month_name[month]] = [h, a, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html#sphx-glr-gallery-lines-bars-and-markers-horizontal-barchart-distribution-py\n",
    "\n",
    "category_names = ['H', 'A', 'D']\n",
    "results = counts\n",
    "\n",
    "labels = list(results.keys())\n",
    "data = np.array(list(results.values()))\n",
    "data_cum = data.cumsum(axis=1)\n",
    "category_colors = plt.colormaps['twilight'](\n",
    "    np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "    widths = data[:, i]\n",
    "    starts = data_cum[:, i] - widths\n",
    "    rects = ax.barh(labels, widths, left=starts, height=0.5,\n",
    "                    label=colname, color=color)\n",
    "\n",
    "    r, g, b, _ = color\n",
    "    text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "    ax.bar_label(rects, label_type='center', fmt='%.1f', color=text_color)\n",
    "    \n",
    "ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 0),\n",
    "            loc='upper left', fontsize='small')\n",
    "plt.title(\"Average match results (%) w.r.t. months across years 2000-2022\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_epl, x='Month', binwidth=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it looks like results across months are the similar, apart from June/July and March. This may be the outcome of much less matches being played during those months. Let's investigate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the number of matches won by a team changes across years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "df_epl[\"Day\"] = df_epl[\"Date\"].dt.day\n",
    "df_epl[\"Month\"] = df_epl[\"Date\"].dt.month \n",
    "df_epl[\"Year\"] = df_epl[\"Date\"].dt.year\n",
    "\n",
    "# Turn the catergorical data into labels\n",
    "df_epl[\"HomeTeam_Enc\"] = df_epl[\"HomeTeam\"].astype(\"category\").cat.codes\n",
    "df_epl[\"AwayTeam_Enc\"] = df_epl[\"AwayTeam\"].astype(\"category\").cat.codes\n",
    "df_epl[\"Referee_Enc\"] = df_epl[\"Referee\"].astype(\"category\").cat.codes\n",
    "\n",
    "FTR_encoder = LabelEncoder()\n",
    "FTR_Enc = FTR_encoder.fit_transform(df_epl[\"FTR\"])\n",
    "df_epl[\"FTR_Enc\"] = FTR_Enc\n",
    "HTR_Enc = FTR_encoder.fit_transform(df_epl[\"HTR\"])\n",
    "df_epl[\"HTR_Enc\"] = HTR_Enc\n",
    "\n",
    "df_epl[\"Home_Manager_Enc\"] = df_epl[\"Home_Manager\"].astype(\"category\").cat.codes\n",
    "df_epl[\"Away_Manager_Enc\"] = df_epl[\"Away_Manager\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Adding Average Past Match Statistics & Past Season % Number Of Wins](#toc0_)\n",
    "\n",
    "_HISTORY -> We add averages of past stats between the specific two teams in question. We obtain these stats (for each row of df_epl) by filtering the df_epl dataframe for matches ONLY between HomeTeam and AwayTeam that took place before the match date. Then take an average of the columns (with stats) like HR, AR, etc (of filtered dataframe). This happens for each row. This will provide us with the average past stats for games played in the past between the specific two teams.\n",
    "\n",
    "_AVG -> We add averages of past stats between the for each of two teams in question. We obtain these stats (for each row of df_epl) by filtering the df_epl dataframe for matches between HomeTeam against ALL other teams that took place before the match date in that current season. Similar is done for the AwayTeam. Then take an average of the columns (with stats) like HR, AR, etc (of the filtered dataframe). This process happens for each row. This will provide us with the average past stats for the all HomeTeam games in that season and all AwayTeam games in that season.\n",
    "\n",
    "HW_AVG & AW_AVG -> The number of past wins are calculated by summing the number of wins by the team in the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - THESE ARE HELPER FUNCTIONS WE NEED:\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if date.month <= 7:\n",
    "        return datetime(date.year-1, 8, 1)\n",
    "    return datetime(date.year-0, 8, 1)\n",
    "\n",
    "def filter_dataframe_by_hometeam_history(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_history(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_hometeam_recent_season(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_recent_season(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "# This function takes as input the filtered dataframe from previous cell, features to average and a dictionary,it then appends an average of each feature to the dictionary\n",
    "def average_columns(avg_features, filtered_df):\n",
    "    for feature in avg_features.keys():\n",
    "        df_col_means = filtered_df[feature].mean()\n",
    "        avg_features[feature].append(df_col_means)\n",
    "        \n",
    "# This function takes as input a filtered dataframe from previous cell, and a list, it then appends the % number of home/away wins in past\n",
    "def find_number_of_wins(number_of_wins_list, filtered_df, team):\n",
    "    df_filtered_ftr = filtered_df.copy()\n",
    "    total_games = df_filtered_ftr.shape[0]\n",
    "    if total_games == 0:\n",
    "        number_of_wins_list.append(np.nan)\n",
    "        return\n",
    "    number_of_wins = df_filtered_ftr[(df_filtered_ftr.FTR==team)].shape[0]\n",
    "    number_of_wins_list.append(number_of_wins/total_games)\n",
    "\n",
    "        \n",
    "# PART 2 - CREATE FEATURES & ADDING THEM TO DATAFRAME:\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_hist = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_hist = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_recent = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_recent = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "number_of_wins_HOME = []\n",
    "number_of_wins_AWAY = []\n",
    "\n",
    "\n",
    "# Run the two functions on each row of the df_epl and fill the dictionary\n",
    "for index, row in df_epl.iterrows():\n",
    "    # Filter the dataframe to only show matches played between those teams and before the certain date\n",
    "    df_epl_train_average_hometeam_history = filter_dataframe_by_hometeam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_history = filter_dataframe_by_awayteam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_hometeam_recent_season = filter_dataframe_by_hometeam_recent_season(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_recent_season = filter_dataframe_by_awayteam_recent_season(df_epl, row[\"Date\"],row[\"AwayTeam_Enc\"])\n",
    "    # Get averages from the filtered dataframe and add to the dictionary\n",
    "    average_columns(avg_features_home_hist, df_epl_train_average_hometeam_history)\n",
    "    average_columns(avg_features_away_hist, df_epl_train_average_awayteam_history)\n",
    "    average_columns(avg_features_home_recent, df_epl_train_average_hometeam_recent_season)\n",
    "    average_columns(avg_features_away_recent, df_epl_train_average_awayteam_recent_season)\n",
    "    # Get number_of_wins from the filtered dataframe and add to list\n",
    "    find_number_of_wins(number_of_wins_HOME, df_epl_train_average_hometeam_recent_season, \"H\")\n",
    "    find_number_of_wins(number_of_wins_AWAY, df_epl_train_average_awayteam_recent_season, \"A\")\n",
    "\n",
    "    \n",
    "# Add features to dataframe\n",
    "for feature in avg_features_home_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "\n",
    "# Add features to dataframe\n",
    "for feature in avg_features_away_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_home_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_away_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "    \n",
    "# Add the past % number of wins\n",
    "df_epl[\"HW_AVG\"] = number_of_wins_HOME\n",
    "df_epl[\"AW_AVG\"] = number_of_wins_AWAY\n",
    "\n",
    "\n",
    "# Drop any rows with nan\n",
    "df_epl = df_epl.dropna()\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - THESE ARE HELPER FUNCTIONS WE NEED:\n",
    "\n",
    "def get_season_start_date(date):\n",
    "    if date.month <= 7:\n",
    "        return datetime(date.year-1, 8, 1)\n",
    "    return datetime(date.year-0, 8, 1)\n",
    "\n",
    "def filter_dataframe_by_hometeam_history(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_history(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Date\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_hometeam_recent_season(df, date, HomeTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.HomeTeam_Enc==HomeTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "def filter_dataframe_by_awayteam_recent_season(df, date, AwayTeam):\n",
    "    # Convert the input string date into datetime\n",
    "    date = pd.to_datetime(date, dayfirst=True)\n",
    "\n",
    "    # Filter the dataframe to include only rows where Dateinput(first day of season) && HomeTeam=input(HomeTeam)\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[(df_filtered.Date<date) & (df_filtered.Date>get_season_start_date(date)) & (df_filtered.AwayTeam_Enc==AwayTeam)]\n",
    "\n",
    "    # Return filtered dataframe\n",
    "    return df_filtered\n",
    "\n",
    "# This function takes as input the filtered dataframe from previous cell, features to average and a dictionary,it then appends an average of each feature to the dictionary\n",
    "def average_columns(avg_features, filtered_df):\n",
    "    for feature in avg_features.keys():\n",
    "        df_col_means = filtered_df[feature].mean()\n",
    "        avg_features[feature].append(df_col_means)\n",
    "        \n",
    "# This function takes as input a filtered dataframe from previous cell, and a list, it then appends the % number of home/away wins in past\n",
    "def find_number_of_wins(number_of_wins_list, filtered_df, team):\n",
    "    df_filtered_ftr = filtered_df.copy()\n",
    "    total_games = df_filtered_ftr.shape[0]\n",
    "    if total_games == 0:\n",
    "        number_of_wins_list.append(np.nan)\n",
    "        return\n",
    "    number_of_wins = df_filtered_ftr[(df_filtered_ftr.FTR==team)].shape[0]\n",
    "    number_of_wins_list.append(number_of_wins/total_games)\n",
    "\n",
    "        \n",
    "# PART 2 - CREATE FEATURES & ADDING THEM TO DATAFRAME:\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_hist = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_hist = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "# These are the features we want to get averages for home team\n",
    "avg_features_home_recent = {\n",
    "                  \"FTHG\": [],\n",
    "                  \"HTHG\": [],\n",
    "                  \"HS\"  : [],\n",
    "                  \"HST\" : [],\n",
    "                  \"HF\"  : [],\n",
    "                  \"HC\"  : [],\n",
    "                  \"HY\"  : [],\n",
    "                  \"HR\"  : []\n",
    "              }\n",
    "\n",
    "# These are the features we want to get averages for away team\n",
    "avg_features_away_recent = {\n",
    "                  \"FTAG\": [],\n",
    "                  \"HTAG\": [],\n",
    "                  \"AS\"  : [],\n",
    "                  \"AST\" : [],\n",
    "                  \"AF\"  : [],\n",
    "                  \"AC\"  : [],\n",
    "                  \"AY\"  : [],\n",
    "                  \"AR\"  : []\n",
    "                }\n",
    "\n",
    "number_of_wins_HOME = []\n",
    "number_of_wins_AWAY = []\n",
    "\n",
    "\n",
    "# Run the two functions on each row of the df_epl and fill the dictionary\n",
    "for index, row in df_epl.iterrows():\n",
    "    # Filter the dataframe to only show matches played between those teams and before the certain date\n",
    "    df_epl_train_average_hometeam_history = filter_dataframe_by_hometeam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_history = filter_dataframe_by_awayteam_history(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_hometeam_recent_season = filter_dataframe_by_hometeam_recent_season(df_epl, row[\"Date\"],row[\"HomeTeam_Enc\"])\n",
    "    df_epl_train_average_awayteam_recent_season = filter_dataframe_by_awayteam_recent_season(df_epl, row[\"Date\"],row[\"AwayTeam_Enc\"])\n",
    "    # Get averages from the filtered dataframe and add to the dictionary\n",
    "    average_columns(avg_features_home_hist, df_epl_train_average_hometeam_history)\n",
    "    average_columns(avg_features_away_hist, df_epl_train_average_awayteam_history)\n",
    "    average_columns(avg_features_home_recent, df_epl_train_average_hometeam_recent_season)\n",
    "    average_columns(avg_features_away_recent, df_epl_train_average_awayteam_recent_season)\n",
    "    # Get number_of_wins from the filtered dataframe and add to list\n",
    "    find_number_of_wins(number_of_wins_HOME, df_epl_train_average_hometeam_recent_season, \"H\")\n",
    "    find_number_of_wins(number_of_wins_AWAY, df_epl_train_average_awayteam_recent_season, \"A\")\n",
    "\n",
    "    \n",
    "# Add features to dataframe\n",
    "for feature in avg_features_home_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "# Add features to dataframe\n",
    "for feature in avg_features_away_hist.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_hist[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_HISTORY\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_home_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_home_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "\n",
    "\n",
    "for feature in avg_features_away_recent.keys():\n",
    "    # Get the list of averages for a certain feature from the dicitonary\n",
    "    feature_vals = avg_features_away_recent[feature]\n",
    "    # Add the list of averages into the dataframe for that certain feature\n",
    "    df_epl.loc[:,feature + \"_AVG\"] = feature_vals\n",
    "    \n",
    "# Add the past % number of wins\n",
    "df_epl[\"HW_AVG\"] = number_of_wins_HOME\n",
    "df_epl[\"AW_AVG\"] = number_of_wins_AWAY\n",
    "\n",
    "\n",
    "# Drop any rows with nan\n",
    "df_epl = df_epl.dropna()\n",
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Adding Expected Goals](#toc0_)\n",
    "\n",
    "The expected goals for each team are calculated using a polynomial regression classifier that is trained using  some of the average past statistics. Then for each row of df_epl, we predict the expected goals for HomeTeam and AwayTeam using the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 - CREATE EXPECTED GOALS REGRESSION MODEL:\n",
    "\n",
    "# Here we aim to create a 'expected or predicted goals for a HomeTeam' feature based upon past average stats\n",
    "min_mse_home = float('inf')\n",
    "min_mse_away = float('inf')\n",
    "\n",
    "# Create the design matrix\n",
    "X_H = df_epl.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'FTHG_AVG', 'HTHG_AVG', 'HS_AVG']].values\n",
    "y_H = df_epl.loc[:,'FTHG'].values\n",
    "X_H_train, X_H_test, y_H_train, y_H_test = model_selection.train_test_split(X_H, y_H, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Similar idea for AwayTeam\n",
    "X_A = df_epl.loc[:,['Day', 'Month', 'AwayTeam_Enc', 'FTAG_AVG', 'HTAG_AVG', 'AS_AVG']].values\n",
    "y_A = df_epl.loc[:,'FTAG'].values\n",
    "X_A_train, X_A_test, y_A_train, y_A_test = model_selection.train_test_split(X_A, y_A, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Here we use a polynomial regression classifier - and select best order:\n",
    "for i in range(1,5):\n",
    "    # Select order\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "\n",
    "    # Transform the features\n",
    "    X_H_train_transform = poly.fit_transform(X_H_train)\n",
    "    X_H_test_transform = poly.fit_transform(X_H_test)\n",
    "\n",
    "    LR_Model_HOME_EG = LinearRegression()\n",
    "    # Fit the model using training data\n",
    "    LR_Model_HOME_EG.fit(X_H_train_transform, y_H_train)\n",
    "    # Make predictions using the model we have created\n",
    "    LR_H_predictions_test = LR_Model_HOME_EG.predict(X_H_test_transform)\n",
    "#     # Check the mean square error(MSE) for HomeTeam Expected Goals\n",
    "#     print(i, mean_squared_error(LR_H_predictions_test, y_H_test))\n",
    "\n",
    "    # Transform the features\n",
    "    X_A_train_transform = poly.fit_transform(X_A_train)\n",
    "    X_A_test_transform = poly.fit_transform(X_A_test)\n",
    "\n",
    "    LR_Model_AWAY_EG = LinearRegression()\n",
    "    # Fit the model using training data\n",
    "    LR_Model_AWAY_EG.fit(X_A_train_transform, y_A_train)\n",
    "    # Make predictions using the model we have created\n",
    "    LR_A_predictions_test = LR_Model_AWAY_EG.predict(X_A_test_transform)\n",
    "#     # Check the mean square error(MSE) for AwayTeam Expected Goals\n",
    "#     print(i, mean_squared_error(LR_A_predictions_test, y_A_test))\n",
    "\n",
    "    curr_mse_home = mean_squared_error(LR_H_predictions_test, y_H_test)\n",
    "    curr_mse_away = mean_squared_error(LR_A_predictions_test, y_A_test)\n",
    "\n",
    "    if curr_mse_home < min_mse_home:\n",
    "        best_poly1 = poly\n",
    "        best_model_home = LR_Model_HOME_EG\n",
    "        min_mse_home = curr_mse_home\n",
    "\n",
    "    if curr_mse_away < min_mse_away:\n",
    "        best_poly2 = poly\n",
    "        best_model_away = LR_Model_AWAY_EG\n",
    "        min_mse_away = curr_mse_away\n",
    "        \n",
    "        \n",
    "# PART 2 - ADD EXPECTED GOALS:\n",
    "\n",
    "# Using the two regression classfiers above, predict the number of goals that the Home and Away teams will score for each row in the dataframe:\n",
    "HomeExGoals = []\n",
    "AwayExGoals = []\n",
    "\n",
    "# For each row, predict the home and away expected goals\n",
    "for index, row in df_epl.iterrows():\n",
    "    X_Home_features = np.array([[row[\"Day\"],row[\"Month\"],row[\"HomeTeam_Enc\"],row[\"FTHG_AVG\"],row[\"HTHG_AVG\"],row[\"HS_AVG\"]]])\n",
    "    X_Away_features = np.array([[row[\"Day\"],row[\"Month\"],row[\"AwayTeam_Enc\"],row[\"FTAG_AVG\"],row[\"HTAG_AVG\"],row[\"AS_AVG\"]]])\n",
    "    # Transform features since we use polynomial regression\n",
    "    X_Home_features_transform = best_poly1.fit_transform(X_Home_features)\n",
    "    X_Away_features_transform = best_poly2.fit_transform(X_Away_features)\n",
    "    # Use the best polynomial classifier - Note the prediction is a 1 by 1 vector\n",
    "    ex_home_goals = best_model_home.predict(X_Home_features_transform)[0]\n",
    "    ex_away_goals = best_model_away.predict(X_Away_features_transform)[0]\n",
    "    # Add prediciton to list\n",
    "    HomeExGoals.append(ex_home_goals)\n",
    "    AwayExGoals.append(ex_away_goals)\n",
    "\n",
    "# Add this data into the dataframe\n",
    "df_epl[\"Ex_Goals_Home\"] = HomeExGoals\n",
    "df_epl[\"Ex_Goals_Away\"] = AwayExGoals\n",
    "\n",
    "\n",
    "# # Turn the catergorical data into labels using same method from before\n",
    "# df_epl[\"AwayTeam_Enc\"] = df_epl[\"AwayTeam\"].astype(\"category\").cat.codes\n",
    "# df_epl[\"HomeTeam_Enc\"] = df_epl[\"HomeTeam\"].astype(\"category\").cat.codes\n",
    "# df_epl = df_epl.drop(['HomeTeam', 'AwayTeam', 'Div'], axis=1)\n",
    "# # Transform the date column into day and month columns and Add into dataframe (Extract days & months from date)\n",
    "# df_epl[\"Date\"] = pd.to_datetime(df_epl[\"Date\"])\n",
    "# df_epl[\"Day\"] = df_epl[\"Date\"].dt.day\n",
    "# df_epl[\"Month\"] = df_epl[\"Date\"].dt.month \n",
    "# df_epl[\"Year\"] = df_epl[\"Date\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Removing Pre-encoded Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_epl.drop(['FTR','HTR','Referee','Home_Manager','Away_Manager'],inplace=True,axis=1)\n",
    "df_epl.drop(['HTR','Referee','Home_Manager','Away_Manager'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Breakdown of Features In The Dataframe/Dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Final Dataframe containing all features](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_epl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_1_'></a>[PLOT COOR MATRIX AGAIN (DEBUG)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(df_epl.corr(numeric_only=\"False\"), cmap=\"PiYG\", annot= False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Auxiliary Functions + Classifier interfaces](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Evaluation helpers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_report(y_pred, y_test):\n",
    "  y_pred = y_pred.ravel()\n",
    "  y_test = y_test.ravel()\n",
    "\n",
    "  print(\"Balanced Accuracy: \", balanced_accuracy_score(y_test,y_pred))\n",
    "  print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "  # handle f1 score zero division\n",
    "  # https://stackoverflow.com/questions/62326735/metrics-f1-warning-zero-division\n",
    "  print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "  ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for summary part of each feature Set\n",
    "model_acc_dict = {\n",
    "  'RG': 0,\n",
    "  'DT': 0,\n",
    "  'RF': 0,\n",
    "  'KNN': 0,\n",
    "  'SVM': 0,\n",
    "  'XGB': 0,\n",
    "  'NN': 0\n",
    "}\n",
    "\n",
    "compare_feature_sets_dict = {\n",
    "  'Feature Set 1': {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0},\n",
    "  'Feature Set 2': {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0},\n",
    "  'Feature Set 3': {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0},\n",
    "  'Feature Set 4': {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0},\n",
    "  'Feature Set 5': {'RG': 0,'DT': 0,'RF': 0,'KNN': 0,'SVM': 0,'XGB': 0,'NN': 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Plotting helpers](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of Models - Feature Set 1\", fig_size=(9, 9)):\n",
    "  model_names = list(model_acc_dict.keys())\n",
    "  '''\n",
    "  model_names = [\"Random Guess\", \"Decision Tree\", \n",
    "                  \"Random Forest\", \"K Nearest Neighbors\",\n",
    "                  \"Support Vector Machine\", \"XGB\",\n",
    "                  \"Nerual Network\"]\n",
    "  '''\n",
    "  \n",
    "  x_label = \"Balanced Accuracy (%)\"\n",
    "  y_label = \"Models trained\"\n",
    "\n",
    "  accs = model_acc_dict.values()\n",
    "\n",
    "  fig = plt.figure(figsize=fig_size)\n",
    "  ax = fig.gca()\n",
    "  p1 = ax.barh(model_names, accs)\n",
    "\n",
    "  ax.set_title(title, fontsize=12)\n",
    "  ax.set_xlabel(x_label)\n",
    "  ax.set_ylabel(y_label)\n",
    "\n",
    "  for i, v in enumerate(accs):\n",
    "      ax.text(v//2, i, str(v), color='white', fontsize=9, ha='left', va='center')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used to plot changes on training loss & cross validation loss\n",
    "'''\n",
    "def plot_train_test_acc(results , scoring, param_x = \"param_max_depth\",title=\"GridSearchCV evaluation\", xlabel=\"max_depth\", ylabel=\"Score\",xlim=(0,100), ylim=(0.4,1), fig_size=(9, 9)):\n",
    "\n",
    "    # REF:https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(title, fontsize=16)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results[param_x].data, dtype=float)\n",
    "\n",
    "    for scorer, color in zip(sorted(scoring), [\"g\", \"k\"]):\n",
    "        for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "            sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
    "            sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
    "            ax.fill_between(\n",
    "                X_axis,\n",
    "                sample_score_mean - sample_score_std,\n",
    "                sample_score_mean + sample_score_std,\n",
    "                alpha=0.1 if sample == \"test\" else 0,\n",
    "                color=color,\n",
    "            )\n",
    "            # change label(test) -> cross validation to avoid confusion\n",
    "            if sample == \"test\":\n",
    "              sample = \"cross validation\"\n",
    "            ax.plot(\n",
    "                X_axis,\n",
    "                sample_score_mean,\n",
    "                style,\n",
    "                color=color,\n",
    "                alpha=1 if sample == \"cross validation\" else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample),\n",
    "            )\n",
    "\n",
    "        best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "        best_score = results[\"mean_test_%s\" % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot(\n",
    "            [\n",
    "                X_axis[best_index],\n",
    "            ]\n",
    "            * 2,\n",
    "            [0, best_score],\n",
    "            linestyle=\"-.\",\n",
    "            color=color,\n",
    "            marker=\"x\",\n",
    "            markeredgewidth=3,\n",
    "            ms=8,\n",
    "        )\n",
    "    \n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.4f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Metrics and classifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_1_'></a>[scoring metrics and define cross validation data split](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: Scoring metrics\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\"\"\"\n",
    "scoring = {\"Accuracy\": \"accuracy\", \"Balanced_accuracy\": \"balanced_accuracy\"}\n",
    "refit = \"Balanced_accuracy\"\n",
    "#NOTE: test on larger splits\n",
    "cv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_1_1_'></a>[helper function for producing report](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval(clf, X_test, y_test, featureset_name, classifier_name):\n",
    "    # Make predictions using the model we have created\n",
    "    y_pred = clf.predict(X_test).ravel()\n",
    "    # Reconverting prediction values (i.e. 0, 1 or 2) back into (H, D or A) using the FTR_encoder defined in earlier cell\n",
    "    y_pred = FTR_encoder.inverse_transform(y_pred)\n",
    "    y_test = y_test.ravel()\n",
    "\n",
    "    evaluate_report(y_pred, y_test)\n",
    "    model_acc_dict[classifier_name] = round(balanced_accuracy_score(y_test,y_pred)*100, 2)\n",
    "    # NOTE: we are not logging values for random guesser here.\n",
    "    compare_feature_sets_dict[featureset_name][classifier_name] = round(balanced_accuracy_score(y_test,y_pred)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_2_'></a>[Classifiers](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_1_'></a>[Random Guesses](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGuessClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Custom implementation of the random guess classifier. \n",
    "    \n",
    "    Compatible with sklearn.metric.plot_confusion_matrix function.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._labels: np.ndarray = None\n",
    "\n",
    "    def fit(self, x_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"Does not use x_train at all (who would actually want to use it lol). \n",
    "        Saves available labels from y_train.\n",
    "        \"\"\"\n",
    "        self._labels = np.unique(y_train)\n",
    "\n",
    "    def predict(self, x_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"For every sample in x_test, chooses a label from self._labels at random\"\"\"\n",
    "        np.random.seed(42)\n",
    "        return np.array(list(map(lambda _: np.random.choice(self._labels, 1), x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rg_n_cv(X_train, y_train):\n",
    "    rgc = RandomGuessClassifier()\n",
    "    rgc.fit(X_train, y_train)\n",
    "    return rgc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_2_'></a>[Decision Tree Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dt_n_cv(X_train, y_train):\n",
    "    # Create an empty Tree model\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    # Fit the model using training data\n",
    "    dt.fit(X_train, y_train)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Decision Tree\"\"\"\n",
    "def fit_DT(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'ccp_alpha': 0.0,\n",
    "    'class_weight': None,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'random_state': None,\n",
    "    'splitter': 'best'}\n",
    "    \"\"\"\n",
    " \n",
    "    classifier = DecisionTreeClassifier(random_state=42)\n",
    "    print(list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)))\n",
    "    param_grid = {'max_depth': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)),\n",
    "                  'max_features': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_3_'></a>[Random Forest Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rf_n_cv(X_train, y_train):\n",
    "    # Create an empty Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    # Fit the model using training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_RF(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'bootstrap': True,\n",
    "    'ccp_alpha': 0.0,\n",
    "    'class_weight': None,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,\n",
    "    'max_features': 'sqrt',\n",
    "    'max_leaf_nodes': None,\n",
    "    'max_samples': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': None,\n",
    "    'oob_score': False,\n",
    "    'random_state': None,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False}\n",
    "    \"\"\"\n",
    "    \n",
    "    classifier = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {'max_depth':list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int)),\n",
    "                  'max_features': list(np.linspace(1, X.shape[1], X.shape[1]//5, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_4_'></a>[K-Nearest Neighbours (KNN) Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn_n_cv(X_train, y_train):\n",
    "    # Create an empty KNN model\n",
    "    knn = KNeighborsClassifier()\n",
    "    # Fit the model using training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_KNN(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'algorithm': 'auto',\n",
    "    'leaf_size': 30,\n",
    "    'metric': 'minkowski',\n",
    "    'metric_params': None,\n",
    "    'n_jobs': None,\n",
    "    'n_neighbors': 5,\n",
    "    'p': 2,\n",
    "    'weights': 'uniform'}\n",
    "    \"\"\"\n",
    "    classifier = KNeighborsClassifier()\n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {'n_neighbors':list(np.linspace(start=3, stop=100, num=50, dtype=int))}\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_5_'></a>[Support Vector Machine (SVM) Classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm_n_cv(X_train, y_train):\n",
    "    # Create an empty svm classifier model with RBF Kernal\n",
    "    svc = svm.SVC(kernel='rbf')\n",
    "    # Fit the model using training data\n",
    "    svc.fit(X_train, y_train)\n",
    "    return svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_SVM(X, y):\n",
    "    \"\"\" Parameters can be tweaked for regularization (more options see sklearn documentation)\n",
    "    {'C': 1.0,\n",
    "    'break_ties': False,\n",
    "    'cache_size': 200,\n",
    "    'class_weight': None,\n",
    "    'coef0': 0.0,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'degree': 3,\n",
    "    'gamma': 'scale',\n",
    "    'kernel': 'rbf',\n",
    "    'max_iter': -1,\n",
    "    'probability': False,\n",
    "    'random_state': None,\n",
    "    'shrinking': True,\n",
    "    'tol': 0.001,\n",
    "    'verbose': False}\n",
    "    \"\"\"\n",
    "    classifier = svm.SVC()\n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {'C':[1.0,5.0,10.0,15.0, 20.0, 25.0, 30.0]\n",
    "                  }\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_6_'></a>[XGB](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgb_n_cv(X_train, y_train):\n",
    "    # create a baseline XGBoost classifier\n",
    "    # changed use_label_encoder=False here \n",
    "\n",
    "    ## NOTE: USE CPU\n",
    "    #xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "\n",
    "    ## NOTE: USE GPU\n",
    "    xgb = XGBClassifier(eval_metric=\"mlogloss\", tree_method=\"gpu_hist\", gpu_id=0)\n",
    "\n",
    "    # Fit the model using training data\n",
    "    xgb.fit(X_train, y_train)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_XGB(X, y):\n",
    "\n",
    "    ## NOTE: USE CPU\n",
    "    #classifier = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "\n",
    "    ## NOTE: USE GPU\n",
    "    classifier = XGBClassifier(eval_metric=\"mlogloss\", tree_method=\"gpu_hist\", gpu_id=0)\n",
    "    \n",
    "    # Only tweaked the depth here as an example\n",
    "    param_grid = {\n",
    "        \"booster\": ['gbtree', 'dart'],\n",
    "        \"learning_rate\": [0.1, 0.3, 0.5],\n",
    "        \"n_estimators\": [5, 10, 20, 50, 100],\n",
    "        \"max_depth\": [5, 10, 20, 50, 100],\n",
    "        \"tree_method\": ['exact', 'approx', 'hist'],\n",
    "        \"eval_metric\": ['mlogloss']\n",
    "    }\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        classifier, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X, y)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_7_'></a>[Neural Network](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc5_3_2_7_1_'></a>[Build function of the NN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(input_size):\n",
    "\n",
    "    # Clean session so when running CV in parallel, it does not cause OOM\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_size, activation='relu', input_dim=input_size),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'],\n",
    "                weighted_metrics=['accuracy']\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_n_cv(X_train, y_train):\n",
    "    # warp this obj into a sklearn classifier\n",
    "    model_warpped = KerasClassifier(model=build_clf, input_size=X_train.shape[1], epochs=20, verbose=1)\n",
    "    #print(model_warpped.get_params()) \n",
    "\n",
    "    # balanced accuracy weights\n",
    "    counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = dict(zip(counts[0], np.reciprocal(counts[1].astype('float64'))))\n",
    "\n",
    "    model_warpped.fit(X_train, y_train, class_weight=class_weights)\n",
    "\n",
    "    return model_warpped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_NN(X_train, y_train):\n",
    "    '''\n",
    "    {'model': <function build_clf at 0x7fad83129870>, \n",
    "    'build_fn': None, 'warm_start': False, \n",
    "    'random_state': None, \n",
    "    'optimizer': 'rmsprop', \n",
    "    'loss': None, \n",
    "    'metrics': None, \n",
    "    'batch_size': None, \n",
    "    'validation_batch_size': None, \n",
    "    'verbose': 1, \n",
    "    'callbacks': None, \n",
    "    'validation_split': 0.0, \n",
    "    'shuffle': True, \n",
    "    'run_eagerly': False, \n",
    "    'epochs': 10, \n",
    "    'input_size': 28, \n",
    "    'class_weight': None}\n",
    "    '''\n",
    "\n",
    "    # warp this obj into a sklearn classifier\n",
    "    model_warpped = KerasClassifier(model=build_clf, input_size=X_train.shape[1], epochs=10)\n",
    "    print(model_warpped.get_params()) \n",
    "\n",
    "    # balanced accuracy weights\n",
    "    counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = dict(zip(counts[0], np.reciprocal(counts[1].astype('float64'))))\n",
    "\n",
    "    param_grid = {\n",
    "        'epochs': [1, 5, 10, 20, 30],\n",
    "        'optimizer': ['rmsprop', 'adam', 'adagrad'],\n",
    "        'batch_size': [8, 16, 64],\n",
    "        'class_weight': [class_weights]\n",
    "    }\n",
    "\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        model_warpped, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X_train, y_train)\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_2_8_'></a>[Feature set interfaces](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc5_3_2_8_1_'></a>[without cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df_without_cv(X_train, y_train, X_test, y_test, featureset_name=\"Feature Set 2\"):\n",
    "\n",
    "    # # fit decision Tree Classifier\n",
    "    # clf = fit_dt_n_cv(X_train, y_train)\n",
    "    # clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"DT\")\n",
    "    \n",
    "    # # fit random forest\n",
    "    # clf = fit_rf_n_cv(X_train, y_train)\n",
    "    # clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"RF\")\n",
    "\n",
    "    # # fit knn\n",
    "    # clf = fit_knn_n_cv(X_train, y_train)\n",
    "    # clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"KNN\")\n",
    "\n",
    "    # # fit SVM\n",
    "    # clf = fit_svm_n_cv(X_train, y_train)\n",
    "    # clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"SVM\")   \n",
    "\n",
    "    # # fit XGB\n",
    "    # clf = fit_xgb_n_cv(X_train, y_train)\n",
    "    # clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"XGB\")  \n",
    "\n",
    "    # fit NN\n",
    "    clf = fit_nn_n_cv(X_train, y_train)\n",
    "    clf_eval(clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"NN\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc5_3_2_8_2_'></a>[with cross-validation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_df_cv(X_train, y_train, X_test, y_test, featureset_name=\"Feature Set 2\"):\n",
    "    \n",
    "    # fit decision Tree Classifier\n",
    "    clf = fit_DT(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring, title=\"Evaluation using Cross Validation (Decision Tree)\", xlabel=\"max_depth\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,20), ylim=(0.2,1.0))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"DT\")\n",
    "\n",
    "    # fit Random Forest Classifier\n",
    "    clf = fit_RF(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring, title=\"Evaluation using Cross Validation (Random Forest)\", xlabel=\"max_depth\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,20), ylim=(0.2,1.0))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"RF\")\n",
    "\n",
    "    # fit KNN\n",
    "    clf = fit_KNN(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_n_neighbors\", title=\"Evaluation using Cross Validation (KNN)\", xlabel=\"number of neighbors\", ylabel=\"Score\", fig_size=(8,8), xlim=(3,100), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"KNN\")\n",
    "\n",
    "    # fit SVM\n",
    "    clf = fit_SVM(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_C\", title=\"Evaluation using Cross Validation (SVM, rbf kernel)\", xlabel=\"C value\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,35), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"SVM\")\n",
    "\n",
    "    # XGB\n",
    "    clf = fit_XGB(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    #plot validation loss vs trainning loss\n",
    "    #plot_train_test_acc()\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"XGB\")\n",
    "\n",
    "    # NN\n",
    "    clf = fit_NN(X_train, y_train)\n",
    "    results = clf.cv_results_\n",
    "    # plot validation loss vs trainning loss\n",
    "    plot_train_test_acc(results, scoring,param_x=\"param_epochs\", title=\"Evaluation using Cross Validation (NN)\", xlabel=\"Number of epoch\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,35), ylim=(0.2,0.7))\n",
    "    best_clf = clf.best_estimator_\n",
    "    clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=\"NN\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Model Selection via Cross Validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_1_'></a>[Introduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we aim to optimize the final few feature sets using model selection and cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_2_'></a>[FEATURE SET 5 (MANUAL) – WITH Model Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc6_1_2_1_'></a>[Create Design Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df_epl):\n",
    "  return df_epl.copy()\n",
    "\n",
    "def create_design_matrix(df):\n",
    "  X = df.loc[:,['Day', 'Month', 'HomeTeam_Enc', 'AwayTeam_Enc',\n",
    "                              'HS_HISTORY','AS_HISTORY','HST_HISTORY','AST_HISTORY','HF_HISTORY','AF_HISTORY','HC_HISTORY','AC_HISTORY','HY_HISTORY','AY_HISTORY','HR_HISTORY','AR_HISTORY',\n",
    "                              'HS_AVG','AS_AVG','HST_AVG','AST_AVG','HF_AVG','AF_AVG','HC_AVG','AC_AVG','HY_AVG','AY_AVG','HR_AVG','AR_AVG','HW_AVG','AW_AVG','Ex_Goals_Home','Ex_Goals_Away',\n",
    "                              'HomeStanding_PrevSeason','AwayStanding_PrevSeason','DiffStanding_PrevSeason','Home_Manager_Enc','Away_Manager_Enc']].values\n",
    "  # X = df.drop(['FTR', 'Date'], axis=1).values\n",
    "  return X\n",
    "\n",
    "df_final = create_df(df_epl)\n",
    "X = create_design_matrix(df_final)\n",
    "y = df_final.loc[:,['FTR']].values.ravel()\n",
    "# NOTE: Shuffle is on!!!\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "FTR_encoder = LabelEncoder()\n",
    "y_train = FTR_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[CONTINUE ON BEST FEATURE SET & EXPLORE MORE ON NN](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results. We select NN as our final classifier model and continue with feature set 5 since it has significant higher balanced accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc6_2_1_1_'></a>[redefine summary info](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefine aux_function for later evaluation(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for summary part of each feature Set\n",
    "model_acc_dict = {\n",
    "'build_clf_0': 0,\n",
    "'build_clf_1': 0\n",
    "}\n",
    "\n",
    "# NOTE: NO EFFECT HERE, needed just because of eval function\n",
    "compare_feature_sets_dict = {\n",
    "'Feature Set 5': {'build_clf_0': 0, 'build_clf_1': 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_2_2_'></a>[Aux Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc6_2_2_1_'></a>[Build function](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we proposed NN with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_0(input_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_size, activation='relu', input_dim=input_size),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'],\n",
    "                weighted_metrics=['accuracy']\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_1(input_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_size, activation='relu', input_dim=input_size),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'],\n",
    "                weighted_metrics=['accuracy']\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf_2(input_size):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(input_size, activation='relu', input_dim=input_size),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'],\n",
    "                weighted_metrics=['accuracy']\n",
    "                )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_NN_tone(X_train, y_train, build_func):\n",
    "    '''\n",
    "    {'model': <function build_clf at 0x7fad83129870>, \n",
    "    'build_fn': None, 'warm_start': False, \n",
    "    'random_state': None, \n",
    "    'optimizer': 'rmsprop', \n",
    "    'loss': None, \n",
    "    'metrics': None, \n",
    "    'batch_size': None, \n",
    "    'validation_batch_size': None, \n",
    "    'verbose': 1, \n",
    "    'callbacks': None, \n",
    "    'validation_split': 0.0, \n",
    "    'shuffle': True, \n",
    "    'run_eagerly': False, \n",
    "    'epochs': 10, \n",
    "    'input_size': 28, \n",
    "    'class_weight': None}\n",
    "    '''\n",
    "\n",
    "    # reset seed, everytime before training\n",
    "    os.environ['PYTHONHASHSEED'] = '42'\n",
    "    tf.keras.utils.set_random_seed(42)   \n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    # warp this obj into a sklearn classifier\n",
    "    model_warpped = KerasClassifier(model=build_func, input_size=X_train.shape[1], epochs=10)\n",
    "    # print(model_warpped.get_params()) \n",
    "\n",
    "    # balanced accuracy weights\n",
    "    counts = np.unique(y_train, return_counts=True)\n",
    "    class_weights = dict(zip(counts[0], np.reciprocal(counts[1].astype('float64'))))\n",
    "\n",
    "    param_grid = {\n",
    "        'epochs': [1, 5, 10, 20, 30, 50],\n",
    "        'optimizer': ['rmsprop', 'adam', 'adagrad'],\n",
    "        'batch_size': [8, 16, 64],\n",
    "        'class_weight': [class_weights]\n",
    "    }\n",
    "\n",
    "    # NOTE: more option eg. custom scoring metric avaliable -> see sklearn doc\n",
    "    # n_jobs used for optimization (use all processor)\n",
    "    # verbose -> display detail (0,1,>1) higher -> more detailed\n",
    "    grid = GridSearchCV(\n",
    "        model_warpped, \n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        verbose=1, \n",
    "        # Assign all cores - 2, otherwise the system may crash(so lagging)\n",
    "        n_jobs=-2, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True,\n",
    "        refit=refit)\n",
    "    clf = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix seed number for NN, so results are reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_NN_tone_CV(X_train, y_train, X_test, y_test, featureset_name=\"Feature Set 5\"):\n",
    "\n",
    "    my_NNs = [build_clf_0, build_clf_1]\n",
    "    for cur_build_func in my_NNs:\n",
    "        clf = fit_NN_tone(X_train, y_train, cur_build_func)\n",
    "        results = clf.cv_results_\n",
    "        # plot validation loss vs trainning loss\n",
    "        plot_train_test_acc(results, scoring,param_x=\"param_epochs\", title=\"Evaluation using Cross Validation (NN)\", xlabel=\"Number of epoch\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,35), ylim=(0.2,0.7))\n",
    "        best_clf = clf.best_estimator_\n",
    "        clf_eval(best_clf, X_test, y_test, featureset_name=featureset_name, classifier_name=str(cur_build_func))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_NN_tone_CV(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hist(model_acc_dict=model_acc_dict, title=\"Summary of NNs - Feature Set 5\", fig_size=(9, 9))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RANDOM FOREST (DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Random Forest Classifier\n",
    "clf = fit_RF(X_train, y_train)\n",
    "results = clf.cv_results_\n",
    "# plot validation loss vs trainning loss\n",
    "plot_train_test_acc(results, scoring, title=\"Evaluation using Cross Validation (Random Forest)\", xlabel=\"max_depth\", ylabel=\"Score\", fig_size=(8,8), xlim=(0,20), ylim=(0.2,1.0))\n",
    "best_clf = clf.best_estimator_\n",
    "clf_eval(best_clf, X_test, y_test, featureset_name=\"Feature Set 5\", classifier_name=\"RF\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WITHOUT CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest\n",
    "clf = fit_rf_n_cv(X_train, y_train)\n",
    "clf_eval(clf, X_test, y_test, featureset_name='Feature Set 5', classifier_name=\"RF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "929de6c31a01da29cb4e64f921de52b26b0f92466a5b3a313ad7d1ab7783989a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
